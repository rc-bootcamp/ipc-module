{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"<code>ipc-module</code>","text":"<p><code>ipc-module</code> is a Python library for measuring and analyzing information processing capacity (IPC), an indicator proposed in reservoir computing. Using <code>pytorch</code> and <code>cupy</code>, it can quickly measure the IPC of various systems on GPUs. Currently, it supports the measurement of IPC for one-dimensional inputs. It is also possible to measure the IPC for arbitrary input time series distributions using Gram-Schmidt orthogonalization.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install ipc-module\n</code></pre> <p>Alternatively, you can use various project management tools (such as <code>uv</code>, <code>poetry</code>, <code>pipenv</code>, etc.) to install it.</p>"},{"location":"#usage","title":"Usage","text":"<p>In short, IPC is a metric that evaluates how much a system can process input time-series information. It generates various linear and nonlinear transformations of input time series using orthogonal polynomials without duplication. It evaluates how well each transformation can be reproduced from the system's current state. For a detailed explanation of IPC and more usage examples of <code>ipc-module</code>, see Chapters 6 and 7 and their sample code at RC bootcamp (URL: https://rc-bootcamp.github.io/), a hands-on resource for reservoir computing beginners.</p>"},{"location":"#measuring-ipc-of-echo-state-network","title":"Measuring IPC of echo state network","text":"<p>The following example shows how to measure the IPC of an echo state network (ESN), a type of recurrent neural network. The following cell generates a 1-input, 100-dimensional ESN and stores its response to a 100,000-step input time series generated from i.i.d. uniform random numbers in [-1, 1] as <code>xs</code>. When you use it, replace this <code>xs</code> with the time series of the system to be measured.</p> <pre><code>import numpy as np\n\nseed = 1234\ninput_dim = 1\nesn_dim = 100\nt_washout = 1000\nt_sample = 100000\nesn_sr = np.linspace(0, 2.0, 21)[1:]  # Spectral radii to test.\n\nrngs = np.random.default_rng(seed).spawn(4)\nx0 = rngs[0].uniform(-1, 1, (len(esn_sr), esn_dim))\nus = rngs[1].uniform(-1, 1, (t_washout + t_sample, input_dim))  # Input signal.\nw_esn = rngs[2].normal(0, 1 / np.sqrt(esn_dim), (esn_dim, esn_dim))\nw_esn /= np.abs(np.linalg.eigvals(w_esn)).max()  # Normalize to spectral radius 1.\nw_in = rngs[3].normal(0, 1, (esn_dim, input_dim))\n\nx = x0\nxs = np.zeros((t_washout + t_sample, *x0.shape), dtype=x0.dtype)\nfor idx in range(t_washout + t_sample):\n    xw = esn_sr[..., None] * (x[..., None, :] @ w_esn.T)[..., 0, :]\n    uw = us[idx : idx + 1] @ w_in.T\n    x = np.tanh(xw + (0.5 * uw + 0.5))  # Asymmetric input.\n    # x = np.tanh(xw + uw)  # Symmetric input.\n    xs[idx] = x\n\nprint(f\"shape of us: {us.shape}\")\nprint(f\"shape of xs: {xs.shape}\")\n</code></pre> <pre><code>shape of us: (101000, 1)\nshape of xs: (101000, 20, 100)\n</code></pre> <p>The response of the ESN, with spectral radius varying from 0.1 to 2.0 in 0.1 steps, is stored, so the shape of <code>xs</code> is <code>(100000, 20, 100)</code>. In the next cell, we measure the IPC for this response <code>xs</code> all at once. Besides <code>numpy</code>, <code>pytorch</code> and <code>cupy</code> can also be used for fast computation on a GPU. This calculation is done using the <code>profiler.UnivariateProfiler</code> class as follows:</p> <pre><code>from ipc_module.config import set_progress_bar\nfrom ipc_module.profiler import UnivariateProfiler\n\nset_progress_bar(False)  # Global setting to hide progress bars.\n\nbackend = \"torch\"\nassert backend in [\"numpy\", \"torch\", \"cupy\"], \"Invalid backend.\"\n\nif backend == \"torch\":\n    import torch\n\n    xs_c, us_c = torch.from_numpy(xs).cuda(), torch.from_numpy(us).cuda()\nelif backend == \"cupy\":\n    import cupy\n\n    xs_c, us_c = cupy.asarray(xs), cupy.asarray(us)\nelse:\n    xs_c, us_c = xs, us\n\n\nprofiler = UnivariateProfiler(\n    us_c,\n    xs_c,\n    poly_name=\"GramSchmidt\",\n    surrogate_num=1000,\n    surrogate_seed=0,\n    offset=t_washout,\n    axis1=0,\n    axis2=-1,\n)\n</code></pre> <p>In IPC, transformations of the input time series are calculated by varying the degree, which represents the strength of nonlinearity, and the delay of the input. The <code>calc</code> method calculates the IPC by specifying the degree and the corresponding maximum delay. The results can be saved in <code>npz</code> format using the <code>save</code> method.</p> <pre><code>degrees = [1, 2, 3, 4, 5, 6]\ntaus = [200, 100, 50, 20, 10, 5]\nfor deg, tau in zip(degrees, taus, strict=True):\n    profiler.calc(deg, tau + 1, zero_offset=True)\nprofiler.save(\"./ipc_esn.npz\", esn_sr=esn_sr, esn_dim=esn_dim)\n</code></pre> <p>You can load saved results with <code>profiler.UnivariateViewer</code>, the parent class of <code>profiler.UnivariateProfiler</code>. But since <code>profiler.UnivariateViewer</code> doesn't possess <code>us</code> or <code>xs</code>, it can't calculate IPC. Use <code>profiler.UnivariateViewer.to_dataframe</code> and <code>helper.visualize_dataframe</code> to visualize IPC results like this:</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0.5)\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0.5)\n\nfrom ipc_module.helper import visualize_dataframe\nfrom ipc_module.profiler import UnivariateViewer\n\nviewer = UnivariateViewer(\"./ipc_esn.npz\")\nesn_sr, esn_dim = viewer.info[\"esn_sr\"], viewer.info[\"esn_dim\"]\ndf, rank = viewer.to_dataframe(max_scale=1.0, truncate_by_rank=True)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nvisualize_dataframe(\n    ax,\n    df,\n    xticks=esn_sr,\n    threshold=0.5,\n    cmap=\"tab10\",\n    group_by=\"component\",  # NOTE: Either \"degree\", \"component\", or \"detail\" is available.\n    fontsize=12,\n)\nax.plot(esn_sr, rank, lw=1.0, color=\"#333333\", marker=\"o\", label=\"Rank\")\nax.legend(fontsize=12, loc=\"upper left\", frameon=False, bbox_to_anchor=(1.0, 1.0), ncol=2)\nax.set_xlabel(\"ESN Spectral Radius\", fontsize=14)\nax.set_title(f\"IPC of {esn_dim}-dim ESN\", fontsize=16)\n\nNone\n</code></pre> <p></p>"},{"location":"#measuring-ipc-of-nonlinear-autoregressive-moving-average-task","title":"Measuring IPC of nonlinear autoregressive moving average task","text":"<p>The example below shows how to measure the IPC for the NARMA10 (nonlinear autoregressive moving average of order 10) task. The NARMA10 task is a nonlinear time-series prediction problem widely used as a benchmark in reservoir computing.</p> <pre><code>def narma_func(us, y_init, alpha=0.3, beta=0.05, gamma=1.5, delta=0.1, mu=0.25, kappa=0.25):\n    assert us.shape[0] &gt; 10\n    vs = mu * us + kappa\n    ys = np.zeros_like(vs)\n    ys[:10] = y_init\n    for idx in range(10, ys.shape[0]):\n        ys[idx] += alpha * ys[idx - 1]\n        ys[idx] += beta * ys[idx - 1] * np.sum(ys[idx - 10 : idx], axis=0)\n        ys[idx] += gamma * vs[idx - 10] * vs[idx - 1]\n        ys[idx] += delta\n    return ys\n\n\nseed = 1234\ninput_dim = 1\nesn_dim = 100\nt_washout = 1000\nt_sample = 100000\namps = np.linspace(0, 0.25, 11)[1:]\n\nrng = np.random.default_rng(seed)\nus = rng.uniform(-1.0, 1.0, (t_sample + t_washout, 1))\nys = narma_func(\n    us[..., None, :], y_init=np.zeros((amps.shape[0], 1)), mu=amps[:, None], kappa=amps[:, None]\n)\n\nprint(f\"shape of us: {us.shape}\")\nprint(f\"shape of ys: {ys.shape}\")\n</code></pre> <pre><code>shape of us: (101000, 1)\nshape of ys: (101000, 10, 1)\n</code></pre> <p>The input time series is generated from a uniform distribution in [-1, 1]. The output time series is generated using the <code>narma_func</code> function. The input distribution is varied by <code>mu</code> and <code>kappa</code> (<code>vs = mu * us + kappa</code>). Typically, <code>mu=0.25</code> and <code>kappa=0.25</code> are used, but here we vary them continuously to examine their effect on IPC.</p> <pre><code>backend = \"torch\"\nassert backend in [\"numpy\", \"torch\", \"cupy\"], \"Invalid backend.\"\n\nif backend == \"torch\":\n    import torch\n\n    ys_c, us_c = torch.from_numpy(ys).cuda(), torch.from_numpy(us).cuda()\nelif backend == \"cupy\":\n    import cupy\n\n    ys_c, us_c = cupy.asarray(ys), cupy.asarray(us)\nelse:\n    ys_c, us_c = ys, us\n\n\nprofiler = UnivariateProfiler(\n    us_c,\n    ys_c,\n    poly_name=\"GramSchmidt\",\n    surrogate_num=1000,\n    surrogate_seed=0,\n    offset=t_washout,\n    axis1=0,\n    axis2=-1,\n)\n\ndegrees = [1, 2, 3, 4, 5, 6]\ntaus = [200, 100, 50, 20, 10, 5]\nfor deg, tau in zip(degrees, taus, strict=True):\n    profiler.calc(deg, tau + 1, zero_offset=True)\nprofiler.save(\"./ipc_narma.npz\", amps=amps)\n</code></pre> <p>The following cell plots the results using the same code as before. This time, we set <code>group_by=\"detail\"</code> to display the IPC for each degree and each delay component, which correspond to the components necessary to solve the NARMA10 task.</p> <pre><code>viewer = UnivariateViewer(\"./ipc_narma.npz\")\namps = viewer.info[\"amps\"]\ndf, rank = viewer.to_dataframe(max_scale=1.0, truncate_by_rank=True)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nvisualize_dataframe(\n    ax,\n    df,\n    xticks=amps,\n    threshold=0.01,\n    cmap=\"tab10\",\n    group_by=\"detail\",  # NOTE: Either \"degree\", \"component\", or \"detail\" is available.\n    fontsize=12,\n)\nax.plot(amps, rank, lw=1.0, color=\"#333333\", marker=\"o\", label=\"Rank\")\nax.legend(fontsize=12, loc=\"upper left\", frameon=False, bbox_to_anchor=(1.0, 1.0), ncol=1)\nax.set_xlabel(\n    r\"$\\sigma$ (Input amplitude, i.e., $v[t] := \\sigma u[t] + \\sigma \\sim \\mathcal{U}\\left([0, \\sigma]\\right)$)\",\n    fontsize=14,\n)\nax.set_title(\"IPC of NARMA10 task\", fontsize=16)\n\nNone\n</code></pre> <p></p>"},{"location":"#license","title":"License","text":"<p><code>ipc-module</code> is provided under the MIT License. You are free to use, modify, and redistribute it. However, you must include the copyright notice and this permission notice in all copies or substantial portions of the software.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions to the development of <code>ipc-module</code>.</p>"},{"location":"#reporting-issues","title":"Reporting issues","text":"<p>If you find any bugs or have suggestions for improvement, please report them on the GitHub Issues page.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use <code>ipc-module</code> in your research or development, please cite the following details about RC bootcamp and its paper. <code>ipc-module</code> is part of RC bootcamp and was released on PyPI at the same time as its publication.</p> <pre><code>@article{inoue2026reservoir,\n  title   = {Reservoir Computing Bootcamp---{{From Python}}/{{NumPy}} Tutorial for the Complete Beginners to Cutting-Edge Research Topics of Reservoir Computing},\n  author  = {Inoue, Katsuma and Kubota, Tomoyuki and Hoan Tran, Quoc and Akashi, Nozomi and Terajima, Ryo and Kabayama, Tempei and Guan, JingChuan and Nakajima, Kohei},\n  year    = 2026,\n  month   = feb,\n  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},\n  volume  = {36},\n  number  = {2},\n  pages   = {023109},\n  issn    = {1054-1500},\n  doi     = {10.1063/5.0283386}\n}\n</code></pre>"},{"location":"#contact","title":"Contact","text":"<p>For questions or feedback about <code>ipc-module</code>, contact us at <code>k-inoue[at]isi.imi.i.u-tokyo.ac.jp</code>.</p>"},{"location":"config/","title":"config","text":""},{"location":"config/#ipc_module.config.set_progress_bar","title":"<code>set_progress_bar(show)</code>","text":"<p>Set the global flag to show or hide the progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>If True, show the progress bar; otherwise, hide it.</p> required Source code in <code>src/ipc_module/config.py</code> <pre><code>def set_progress_bar(show: bool):\n    \"\"\"\n    Set the global flag to show or hide the progress bar.\n\n    Parameters:\n        show (bool): If True, show the progress bar; otherwise, hide it.\n    \"\"\"\n    global SHOW_PROGRESS_BAR\n    SHOW_PROGRESS_BAR = show\n</code></pre>"},{"location":"helper/","title":"helper","text":""},{"location":"helper/#ipc_module.helper.visualize_dataframe","title":"<code>visualize_dataframe(ax, df, xticks=None, group_by='degree', threshold=0.5, sort_by=np.nanmax, cmap='tab10', x_offset=0, min_color_coef=0.5, fontsize=12, step_linewidth=0.5, bottom_min=None, zero_offset=True)</code>","text":"<p>Visualizes IPC results stored in a DataFrame using a bar plot.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Matplotlib Axes object to plot on.</p> required <code>df</code> <code>DataFrame</code> <p><code>polars.DataFrame</code> containing IPC results.</p> required <code>xticks</code> <code>list | ndarray | None</code> <p>X-axis tick positions. If <code>None</code>, default positions are used.</p> <code>None</code> <code>group_by</code> <code>str</code> <p>Grouping method for IPC components. Choose from 'degree', 'component', or 'detail'.</p> <code>'degree'</code> <code>threshold</code> <code>float</code> <p>Threshold for displaying IPC components. Components with values below this threshold are grouped into 'rest'.</p> <code>0.5</code> <code>sort_by</code> <code>callable</code> <p>Function to sort IPC components.</p> <code>nanmax</code> <code>cmap</code> <code>str | Colormap</code> <p>Colormap for coloring IPC components.</p> <code>'tab10'</code> <code>x_offset</code> <code>float</code> <p>Horizontal offset for the x-axis.</p> <code>0</code> <code>min_color_coef</code> <code>float</code> <p>Minimum color coefficient for coloring. Only used when <code>group_by</code> is 'component' or 'detail'.</p> <code>0.5</code> <code>fontsize</code> <code>int</code> <p>Font size for labels.</p> <code>12</code> <code>step_linewidth</code> <code>float</code> <p>Line width for step lines. If 0, no lines are drawn.</p> <code>0.5</code> <code>bottom_min</code> <code>ndarray | None</code> <p>Minimum bottom values for bars.</p> <code>None</code> <code>zero_offset</code> <code>bool</code> <p>Whether the delay offset starts from zero.</p> <code>True</code> Notes <p><code>group_by</code> determines how IPC components are grouped and colored:</p> <ul> <li><code>'degree'</code>: Groups by sum of degrees (e.g., <code>3</code> for <code>(3,)</code>, <code>(2, 1)</code>, <code>(1, 1, 1)</code>).</li> <li><code>'component'</code>: Groups by tuple of degrees (e.g., <code>(3,)</code>, <code>(2, 1)</code>, <code>(1, 1, 1)</code> are distinct).</li> <li><code>'detail'</code>: Groups by tuple of degrees and delays.</li> </ul> <p>Since the number of unique components can grow rapidly, using <code>'component'</code> or <code>'detail'</code> may result in many distinct colors, making it time-consuming to render. Especially for <code>'detail'</code>, consider setting a higher threshold to limit the number of displayed components (e.g., <code>threshold=1.0</code>). Use a positive <code>threshold</code> value to group less significant components into a <code>rest</code> category.</p> Source code in <code>src/ipc_module/helper.py</code> <pre><code>def visualize_dataframe(\n    ax: Axes,\n    df: pl.DataFrame,\n    xticks: list | np.ndarray | None = None,\n    group_by: str = \"degree\",\n    threshold: float = 0.5,\n    sort_by: callable = np.nanmax,\n    cmap: str | plt.Colormap = \"tab10\",\n    x_offset: float = 0,\n    min_color_coef: float = 0.5,\n    fontsize: int = 12,\n    step_linewidth: float = 0.5,\n    bottom_min: np.ndarray | None = None,\n    zero_offset: bool = True,\n):\n    \"\"\"\n\n    Visualizes IPC results stored in a DataFrame using a bar plot.\n\n    Parameters:\n        ax (Axes): Matplotlib Axes object to plot on.\n        df (pl.DataFrame): `polars.DataFrame` containing IPC results.\n        xticks (list | np.ndarray | None, optional): X-axis tick positions. If `None`, default positions are used.\n        group_by (str, optional): Grouping method for IPC components. Choose from 'degree', 'component', or 'detail'.\n        threshold (float, optional): Threshold for displaying IPC components. Components with values below this threshold are grouped into 'rest'.\n        sort_by (callable, optional): Function to sort IPC components.\n        cmap (str | plt.Colormap, optional): Colormap for coloring IPC components.\n        x_offset (float, optional): Horizontal offset for the x-axis.\n        min_color_coef (float, optional): Minimum color coefficient for coloring. Only used when `group_by` is 'component' or 'detail'.\n        fontsize (int, optional): Font size for labels.\n        step_linewidth (float, optional): Line width for step lines. If 0, no lines are drawn.\n        bottom_min (np.ndarray | None, optional): Minimum bottom values for bars.\n        zero_offset (bool, optional): Whether the delay offset starts from zero.\n\n    Notes:\n        `group_by` determines how IPC components are grouped and colored:\n\n        - `'degree'`: Groups by sum of degrees (e.g., `3` for `(3,)`, `(2, 1)`, `(1, 1, 1)`).\n        - `'component'`: Groups by tuple of degrees (e.g., `(3,)`, `(2, 1)`, `(1, 1, 1)` are distinct).\n        - `'detail'`: Groups by tuple of degrees and delays.\n\n        Since the number of unique components can grow rapidly, using `'component'` or `'detail'` may result in many distinct colors, making it time-consuming to render.\n        Especially for `'detail'`, consider setting a higher threshold to limit the number of displayed components (e.g., `threshold=1.0`).\n        Use a positive `threshold` value to group less significant components into a `rest` category.\n    \"\"\"\n\n    ipc_columns = [column for column in df.columns if \"ipc\" in column]\n    assert group_by in [\"degree\", \"component\", \"detail\"], \"invalid `group_by` argments\"\n    col_cmp = sorted([column for column in df.columns if column.startswith(\"cmp\")])\n    col_del = sorted([column for column in df.columns if column.startswith(\"del\")])\n    group_by_columns = dict(degree=[\"degree\"], component=col_cmp, detail=col_cmp + col_del)\n    if type(cmap) is str:\n        cmap = plt.get_cmap(cmap)\n\n    def shape_segment(segment, get_delay=False):\n        if group_by == \"degree\":\n            return tuple(segment)\n        elif group_by == \"component\":\n            return tuple(val for val in segment if val &gt;= 0)\n        elif group_by == \"detail\":\n            degrees = tuple(val for val in segment[: len(segment) // 2] if val &gt;= 0)\n            if get_delay:\n                delays = tuple(val for val in segment[len(segment) // 2 :] if val &gt;= 0)\n                return degrees, delays\n            else:\n                return degrees\n\n    def get_color_index(segment):\n        if group_by == \"degree\":\n            return segment[0], 0, 1\n        elif group_by == \"component\":\n            degrees = shape_segment(segment)\n            degree = sum(degrees)\n            degree_list = make_degree_list(degree)\n            index = dict(zip(degree_list[::-1], range(len(degree_list)), strict=False))[degrees]\n            max_index = len(degree_list)\n            return degree, index, max_index\n        elif group_by == \"detail\":\n            degrees, delays = shape_segment(segment, get_delay=True)\n            degree = sum(degrees)\n            degree_list = make_degree_list(degree)\n            index = dict(zip(degree_list[::-1], range(len(degree_list)), strict=False))[degrees]\n            max_index = len(degree_list)\n            return degree, index + max(0, 1 - 0.9 ** (max(delays) - (not zero_offset))), max_index\n\n    def color_func(segment):\n        white = np.ones(4)\n        degree, index, max_index = get_color_index(segment)\n        coef = (index / max_index) * min_color_coef\n        out = np.array(cmap(degree - 1))\n        out = (1 - coef) * out + coef * white\n        return out\n\n    def label_func(segment):\n        if group_by == \"degree\":\n            return str(segment[0])\n        elif group_by == \"component\":\n            out_str = str(shape_segment(segment))\n            return out_str.replace(\"(\", \"{\").replace(\",)\", \"}\").replace(\")\", \"}\")\n        elif group_by == \"detail\":\n            degrees, delays = shape_segment(segment, get_delay=True)\n            out_str = str(tuple(zip(degrees, delays, strict=False)))\n            return out_str.replace(\"(\", \"{\").replace(\",)\", \"}\").replace(\")\", \"}\")\n\n    def hatch_func(segment):\n        hatches = [\"//\", \"\\\\\\\\\", \"||\", \"--\", \"++\", \"xx\", \"oo\", \"OO\", \"..\", \"**\"]\n        if group_by == \"degree\":\n            return None\n        elif group_by == \"component\":\n            return None\n        elif group_by == \"detail\":\n            _degrees, delays = shape_segment(segment, get_delay=True)\n            return hatches[(max(delays) - (not zero_offset)) % len(hatches)]\n\n    def sort_func(arg):\n        segment, val = arg\n        if sort_by(val) &gt; threshold:\n            if group_by == \"degree\":\n                return segment\n            elif group_by == \"component\":\n                degrees = shape_segment(segment)\n                return (sum(degrees), *(-d for d in degrees))\n            elif group_by == \"detail\":\n                degrees = shape_segment(segment)\n                return (\n                    sum(degrees),\n                    *(-s for s in segment[: (len(segment) // 2)]),\n                    *segment[(len(segment) // 2) :],\n                )\n        else:\n            return (np.inf,)\n\n    # Aggregation process.\n    out = defaultdict(list)\n    segments = df[group_by_columns[group_by]].unique()\n    for column in ipc_columns:\n        df_agg = df.group_by(group_by_columns[group_by]).agg(pl.col(column).sum())\n        for segment in segments.iter_rows():\n            out[segment].append(0)\n        for *segment, val in df_agg.iter_rows():\n            out[tuple(segment)][-1] = val\n\n    # Visualization process.\n    bottom, rest, legend_cnt = 0.0, 0.0, 1\n    if xticks is None:\n        pos = x_offset + np.arange(-1, len(ipc_columns) + 1)\n        width = 1.0\n    else:\n        pos = np.zeros(len(ipc_columns) + 2)\n        pos[1:-1] = xticks\n        width = pos[1] - pos[0]\n        pos[0] = pos[1] - width\n        pos[-1] = pos[-2] + width\n\n    legend_cnt = 1\n    bottom = np.zeros_like(pos, dtype=float)\n    rest = np.zeros_like(pos, dtype=float)\n    for segment, val in sorted(out.items(), key=sort_func):\n        ipc = np.zeros_like(bottom)\n        ipc[1:-1] = val\n        if sort_by(ipc) &gt; threshold:\n            ax.bar(\n                pos[1:-1],\n                ipc[1:-1],\n                bottom=bottom[1:-1] if bottom_min is None else np.maximum(bottom[1:-1], bottom_min),\n                width=width,\n                linewidth=0.0,\n                label=label_func(segment),\n                color=color_func(segment),\n                hatch=hatch_func(segment),\n            )\n            if step_linewidth &gt; 0:\n                ax.step(\n                    pos,\n                    ipc + bottom if bottom_min is None else np.maximum(ipc + bottom, bottom_min),\n                    \"#333333\",\n                    where=\"mid\",\n                    linewidth=step_linewidth,\n                )\n            legend_cnt += 1\n            bottom += ipc\n        else:\n            rest += ipc\n    if threshold &gt; 0:\n        ax.bar(\n            pos[1:-1],\n            rest[1:-1],\n            bottom=bottom[1:-1],\n            width=width,\n            label=\"rest\",\n            color=\"darkgray\",\n            hatch=\"/\",\n            linewidth=0.0,\n        )\n        if step_linewidth &gt; 0:\n            ax.step(pos, rest + bottom, \"#333333\", where=\"mid\", linewidth=step_linewidth)\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=fontsize)\n    ax.legend(\n        loc=\"upper left\",\n        bbox_to_anchor=(1.05, 1.0),\n        borderaxespad=0,\n        ncol=math.ceil(legend_cnt / 18),\n        fontsize=fontsize,\n    )\n    return out\n</code></pre>"},{"location":"polynomial/","title":"polynomial","text":""},{"location":"polynomial/#ipc_module.polynomial.BasePolynomial","title":"<code>BasePolynomial</code>","text":"<p>               Bases: <code>object</code></p> <p>Base class for polynomial classes using recurrence relations.</p> Notes <p>When implementing a new polynomial type, follow these guidelines:</p> <ul> <li>Inherit <code>BasePolynomial</code>.</li> <li><code>calc</code> method should be overridden in subclasses.</li> <li><code>super().__init__</code> should be called in the subclass constructor.</li> </ul> Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>class BasePolynomial(object):\n    \"\"\"\n    Base class for polynomial classes using recurrence relations.\n\n    Notes:\n        When implementing a new polynomial type, follow these guidelines:\n\n        - Inherit `BasePolynomial`.\n        - `calc` method should be overridden in subclasses.\n        - `super().__init__` should be called in the subclass constructor.\n    \"\"\"\n\n    def __init__(self, xs, **_kwargs):\n        self.xs = xs\n        self.caches = {}\n\n    def __getitem__(self, args):\n        if type(args) is tuple:\n            deg, sli = args[0], args[1:]\n        else:\n            deg, sli = args, ()\n        assert deg &gt;= 0\n        if deg not in self.caches:\n            self.caches[deg] = self.calc(deg)\n        if len(sli) &gt; 0:\n            return self.caches[deg][sli]\n        else:\n            return self.caches[deg]\n\n    def calc(self, *args, **kwargs):\n        raise NotImplementedError\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.GramSchmidt","title":"<code>GramSchmidt</code>","text":"<p>               Bases: <code>BasePolynomial</code></p> <p>Gram-Schmidt polynomial class using the Gram-Schmidt process.</p> Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>class GramSchmidt(BasePolynomial):\n    \"\"\"\n    Gram-Schmidt polynomial class using the Gram-Schmidt process.\n    \"\"\"\n\n    def __init__(self, xs, axis=None, depth=None, **_kwargs):\n        \"\"\"\n        Parameters:\n            xs (Any): Input values.\n            axis (int | None, optional): Axis along which to perform the Gram-Schmidt process.\n            depth (int | None, optional): Depth of orthogonalization. If `None`, full depth is used.\n\n        Notes:\n            If `axis` is `None`, be cautious when `xs` is multidimensional, as it might cause unexpected behavior.\n            `axis=-2` is specified by the `BatchRegressor` class since the time dimension is the second-to-last dimension.\n        \"\"\"\n        super(GramSchmidt, self).__init__(xs)\n        if axis is None:\n            warnings.warn(\n                \"Note that axis is set to `None`. Be careful when xs is multidimensional.\",\n                stacklevel=2,\n            )\n        self.axis = axis\n        self.depth = depth\n        self.caches[0] = 1\n\n    def offset(self, n: int):\n        return max(1, 1 if self.depth is None else n - self.depth)\n\n    def calc(self, n: int):\n        if n &gt; 1:\n            base = self[1] * self[n - 1]\n        else:\n            base = self.xs\n        out = base - base.mean(axis=self.axis, keepdims=True)\n        for i in range(self.offset(n), n):\n            out -= (base * self[i]).sum(axis=self.axis, keepdims=True) * self[i]\n        out *= (out * out).sum(axis=self.axis, keepdims=True) ** (-0.5)\n        return out\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.GramSchmidt.__init__","title":"<code>__init__(xs, axis=None, depth=None, **_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xs</code> <code>Any</code> <p>Input values.</p> required <code>axis</code> <code>int | None</code> <p>Axis along which to perform the Gram-Schmidt process.</p> <code>None</code> <code>depth</code> <code>int | None</code> <p>Depth of orthogonalization. If <code>None</code>, full depth is used.</p> <code>None</code> Notes <p>If <code>axis</code> is <code>None</code>, be cautious when <code>xs</code> is multidimensional, as it might cause unexpected behavior. <code>axis=-2</code> is specified by the <code>BatchRegressor</code> class since the time dimension is the second-to-last dimension.</p> Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>def __init__(self, xs, axis=None, depth=None, **_kwargs):\n    \"\"\"\n    Parameters:\n        xs (Any): Input values.\n        axis (int | None, optional): Axis along which to perform the Gram-Schmidt process.\n        depth (int | None, optional): Depth of orthogonalization. If `None`, full depth is used.\n\n    Notes:\n        If `axis` is `None`, be cautious when `xs` is multidimensional, as it might cause unexpected behavior.\n        `axis=-2` is specified by the `BatchRegressor` class since the time dimension is the second-to-last dimension.\n    \"\"\"\n    super(GramSchmidt, self).__init__(xs)\n    if axis is None:\n        warnings.warn(\n            \"Note that axis is set to `None`. Be careful when xs is multidimensional.\",\n            stacklevel=2,\n        )\n    self.axis = axis\n    self.depth = depth\n    self.caches[0] = 1\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.Hermite","title":"<code>Hermite</code>","text":"<p>               Bases: <code>BasePolynomial</code></p> <p>Hermite polynomial class using recurrence relation (Cf. Wikipedia).</p> Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>class Hermite(BasePolynomial):\n    \"\"\"\n    Hermite polynomial class using recurrence relation\n    (Cf. [Wikipedia](https://en.wikipedia.org/wiki/Hermite_polynomials#Recurrence_relation)).\n    \"\"\"\n\n    def __init__(self, xs, normalize: bool = False, **_kwargs):\n        \"\"\"\n        Parameters:\n            xs (Any): Input values.\n            normalize (bool, optional): Whether to use normalized Hermite polynomials.\n        \"\"\"\n        super(Hermite, self).__init__(xs)\n        if normalize:\n            exp = math.e ** (-0.25 * (self.xs * self.xs))\n            exp *= (2 * math.pi) ** -0.25\n            self.caches[0] = exp\n            self.caches[1] = exp * self.xs\n        else:\n            self.caches[0] = 1\n            self.caches[1] = self.xs\n\n    def calc(self, n: int):\n        # res = self.xs * self[n - 1]\n        # res -= (n - 1) * self[n - 2]\n        res = math.sqrt(1 / n) * self.xs * self[n - 1]\n        res -= math.sqrt((n - 1) / n) * self[n - 2]\n        return res\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.Hermite.__init__","title":"<code>__init__(xs, normalize=False, **_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xs</code> <code>Any</code> <p>Input values.</p> required <code>normalize</code> <code>bool</code> <p>Whether to use normalized Hermite polynomials.</p> <code>False</code> Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>def __init__(self, xs, normalize: bool = False, **_kwargs):\n    \"\"\"\n    Parameters:\n        xs (Any): Input values.\n        normalize (bool, optional): Whether to use normalized Hermite polynomials.\n    \"\"\"\n    super(Hermite, self).__init__(xs)\n    if normalize:\n        exp = math.e ** (-0.25 * (self.xs * self.xs))\n        exp *= (2 * math.pi) ** -0.25\n        self.caches[0] = exp\n        self.caches[1] = exp * self.xs\n    else:\n        self.caches[0] = 1\n        self.caches[1] = self.xs\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.Jacobi","title":"<code>Jacobi</code>","text":"<p>               Bases: <code>BasePolynomial</code></p> <p>Jacobi polynomial class using recurrence relation (Cf. Wikipedia).</p> Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>class Jacobi(BasePolynomial):\n    \"\"\"\n    Jacobi polynomial class using recurrence relation\n    (Cf. [Wikipedia](https://en.wikipedia.org/wiki/Jacobi_polynomials#Recurrence_relation)).\n    \"\"\"\n\n    def __init__(self, xs, a: float, b: float, **_kwargs):\n        \"\"\"\n        Parameters:\n            xs (Any): Input values.\n            a (float): Parameter a of the Jacobi polynomial.\n            b (float): Parameter b of the Jacobi polynomial.\n        \"\"\"\n        super(Jacobi, self).__init__(xs)\n        self.a, self.b = a, b\n        self.caches[0] = 1\n        self.caches[1] = self.xs * (a + b + 2) / 2 + (a - b) / 2\n\n    def calc(self, n: int):\n        a = n + self.a\n        b = n + self.b\n        c = a + b\n        d = 2 * n * (c - n) * (c - 2)\n        e = (c - 1) * (c - 2) * c\n        f = (c - 1) * (c - 2 * n) * (a - b)\n        g = -2 * (a - 1) * (b - 1) * c\n\n        res = (e / d) * self.xs * self[n - 1]\n        res += (f / d) * self[n - 1]\n        res += (g / d) * self[n - 2]\n        return res\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.Jacobi.__init__","title":"<code>__init__(xs, a, b, **_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xs</code> <code>Any</code> <p>Input values.</p> required <code>a</code> <code>float</code> <p>Parameter a of the Jacobi polynomial.</p> required <code>b</code> <code>float</code> <p>Parameter b of the Jacobi polynomial.</p> required Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>def __init__(self, xs, a: float, b: float, **_kwargs):\n    \"\"\"\n    Parameters:\n        xs (Any): Input values.\n        a (float): Parameter a of the Jacobi polynomial.\n        b (float): Parameter b of the Jacobi polynomial.\n    \"\"\"\n    super(Jacobi, self).__init__(xs)\n    self.a, self.b = a, b\n    self.caches[0] = 1\n    self.caches[1] = self.xs * (a + b + 2) / 2 + (a - b) / 2\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.Krawtchouk","title":"<code>Krawtchouk</code>","text":"<p>               Bases: <code>BasePolynomial</code></p> <p>Krawtchouk polynomial class using three-term recurrence relation (Cf. Wikipedia).</p> Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>class Krawtchouk(BasePolynomial):\n    \"\"\"\n    Krawtchouk polynomial class using three-term recurrence relation\n    (Cf. [Wikipedia](https://en.wikipedia.org/wiki/Kravchuk_polynomials#Three_term_recurrence)).\n    \"\"\"\n\n    def __init__(self, xs, N=2, p=0.5, **_kwargs):\n        \"\"\"\n        Parameters:\n            xs (Any): Input values.\n            N (int, optional): Parameter N of the Krawtchouk polynomial.\n            p (float, optional): Parameter p of the Krawtchouk polynomial.\n        \"\"\"\n        super(Krawtchouk, self).__init__(xs)\n        self.N, self.p = N, p\n        self.caches[0] = 1\n        self.caches[1] = 1 - self.xs * (1 / (self.N * self.p))\n\n    def calc(self, n: int):\n        assert n &lt;= self.N, f\"argument should be equal or less than N={self.N}, but {n} was given.\"\n        res = (self.p * (self.N - n + 1) + (n - 1) * (1 - self.p) - self.xs) * self[n - 1]\n        res -= (n - 1) * (1 - self.p) * self[n - 2]\n        res /= self.p * (self.N - n + 1)\n        return res\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.Krawtchouk.__init__","title":"<code>__init__(xs, N=2, p=0.5, **_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xs</code> <code>Any</code> <p>Input values.</p> required <code>N</code> <code>int</code> <p>Parameter N of the Krawtchouk polynomial.</p> <code>2</code> <code>p</code> <code>float</code> <p>Parameter p of the Krawtchouk polynomial.</p> <code>0.5</code> Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>def __init__(self, xs, N=2, p=0.5, **_kwargs):\n    \"\"\"\n    Parameters:\n        xs (Any): Input values.\n        N (int, optional): Parameter N of the Krawtchouk polynomial.\n        p (float, optional): Parameter p of the Krawtchouk polynomial.\n    \"\"\"\n    super(Krawtchouk, self).__init__(xs)\n    self.N, self.p = N, p\n    self.caches[0] = 1\n    self.caches[1] = 1 - self.xs * (1 / (self.N * self.p))\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.Laguerre","title":"<code>Laguerre</code>","text":"<p>               Bases: <code>BasePolynomial</code></p> <p>Laguerre polynomial class using recurrence relation (Cf. Wikipedia).</p> Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>class Laguerre(BasePolynomial):\n    \"\"\"\n    Laguerre polynomial class using recurrence relation\n    (Cf. [Wikipedia](https://en.wikipedia.org/wiki/Laguerre_polynomials#Generalized_Laguerre_polynomials)).\n    \"\"\"\n\n    def __init__(self, xs, a=0, **_kwargs):\n        \"\"\"\n        Parameters:\n            xs (Any): Input values.\n            a (float, optional): Parameter a of the Laguerre polynomial.\n        \"\"\"\n        super(Laguerre, self).__init__(xs)\n        self.a = a\n        self.caches[0] = 1\n        self.caches[1] = (a + 1) - self.xs\n\n    def __getitem__(self, n: int):\n        a = self.a\n        res = ((2 * n - 1 + a) / n) * self[n - 1]\n        res -= (1 / n) * self.xs * self[n - 1]\n        res -= ((n - 1 + a) / n) * self[n - 2]\n        return res\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.Laguerre.__init__","title":"<code>__init__(xs, a=0, **_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xs</code> <code>Any</code> <p>Input values.</p> required <code>a</code> <code>float</code> <p>Parameter a of the Laguerre polynomial.</p> <code>0</code> Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>def __init__(self, xs, a=0, **_kwargs):\n    \"\"\"\n    Parameters:\n        xs (Any): Input values.\n        a (float, optional): Parameter a of the Laguerre polynomial.\n    \"\"\"\n    super(Laguerre, self).__init__(xs)\n    self.a = a\n    self.caches[0] = 1\n    self.caches[1] = (a + 1) - self.xs\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.Legendre","title":"<code>Legendre</code>","text":"<p>               Bases: <code>Jacobi</code></p> <p>Legendre polynomial is a special case of Jacobi polynomial with <code>a = b = 0</code> (Cf. Wikipedia).</p> Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>class Legendre(Jacobi):\n    \"\"\"\n    Legendre polynomial is a special case of Jacobi polynomial with `a = b = 0`\n    (Cf. [Wikipedia](https://en.wikipedia.org/wiki/Legendre_polynomials)).\n    \"\"\"\n\n    def __init__(self, xs, **_kwargs):\n        \"\"\"\n        Parameters:\n            xs (Any): Input values.\n        \"\"\"\n        super(Legendre, self).__init__(xs, 0, 0)\n</code></pre>"},{"location":"polynomial/#ipc_module.polynomial.Legendre.__init__","title":"<code>__init__(xs, **_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xs</code> <code>Any</code> <p>Input values.</p> required Source code in <code>src/ipc_module/polynomial.py</code> <pre><code>def __init__(self, xs, **_kwargs):\n    \"\"\"\n    Parameters:\n        xs (Any): Input values.\n    \"\"\"\n    super(Legendre, self).__init__(xs, 0, 0)\n</code></pre>"},{"location":"profiler/","title":"profiler","text":""},{"location":"profiler/#ipc_module.profiler.UnivariateProfiler","title":"<code>UnivariateProfiler</code>","text":"<p>               Bases: <code>UnivariateViewer</code></p> <p>Profiler class to calculate the IPCs for univariate input time series.</p> Notes <p>This class inherits from <code>UnivariateViewer</code>, enabling it to load and convert profiling results into a DataFrame. It stores <code>us</code> and <code>xs</code>, allowing IPC calculations via the <code>calc</code> method. However, it does not support saving or loading time series data directly, as they can be large.</p> Source code in <code>src/ipc_module/profiler.py</code> <pre><code>class UnivariateProfiler(UnivariateViewer):\n    \"\"\"\n    Profiler class to calculate the IPCs for univariate input time series.\n\n    Notes:\n        This class inherits from `UnivariateViewer`, enabling it to load and convert profiling results into a DataFrame.\n        It stores `us` and `xs`, allowing IPC calculations via the `calc` method.\n        However, it does not support saving or loading time series data directly, as they can be large.\n    \"\"\"\n\n    def __init__(\n        self,\n        us,\n        xs,\n        poly_name: str = \"GramSchmidt\",\n        poly_param: dict | None = None,\n        surrogate_num: int = 1000,\n        surrogate_seed: int = 0,\n        axis1: int = 0,\n        axis2: int = -1,\n        **regressor_kws,\n    ):\n        \"\"\"\n        Parameters:\n            us (Any): Input time series. Must be at least 2D and univariate (shape: [t, ..., 1]).\n            xs (Any): Output time series. Must be at least 2D (shape: [t, ..., N]).\n            poly_name (str, optional): Polynomial class name to use, selectable from the `polynomial` module.\n            poly_param (dict | None, optional): Parameters for the polynomial class.\n            surrogate_num (int, optional): Number of surrogate datasets to generate.\n            surrogate_seed (int, optional): Random seed for generating surrogate data.\n            axis1 (int, optional): Axis for time steps; `us` and `xs` must match along this axis.\n            axis2 (int, optional): Axis for variables; defaults to -1 (last axis) for both `us` and `xs`.\n            regressor_kws (dict): Additional arguments for the `BatchRegressor` class.\n\n        Notes:\n            Currently supported backends are NumPy, CuPy, and PyTorch.\n            The following conditions must be met for `us` and `xs`:\n\n            - Both must use the same backend.\n            - Both must be at least 2D arrays.\n            - `axis1` and `axis2` must be different.\n            - They must have the same size along `axis1` (time steps).\n            - `us` must be univariate along `axis2` (i.e., size 1).\n\n            `GramSchmidt` is recommended as the default polynomial class.\n            If you know the specific distribution of `us`, other polynomial classes may perform better.\n            See the table below for the correspondence between distributions and polynomial classes\n            (Reference: [D. Xiu et al. 2002](https://epubs.siam.org/doi/10.1137/S1064827501387826)):\n\n            | Distribution of `us` | Polynomial | Support |\n            |-|-|-|\n            | Normal (Gaussian) | `Hermite` | (-\u221e, \u221e) |\n            | Gamma | `Laguerre` | [0, \u221e) |\n            | Beta | `Jacobi` | [0, 1] |\n            | Uniform | `Legendre` | [-1, 1] |\n            | Binomial | `Krawtchouk` | {0, 1, ..., n} |\n\n            `BatchRegressor` is used internally to compute IPCs in batches, allowing efficient processing of large datasets.\n            The keyword arguments for `BatchRegressor` can be passed via `**regressor_kws`.\n\n            | Option | Description | Default |\n            |-|-|-|\n            | `offset` | The offset applied to the time series data. | `1000` |\n            | `debias` | If `True`, removes the mean from the data. | `True` |\n            | `normalize` | If `True`, scales the data to unit variance. | `False` |\n            | `threshold_mode` | Determines the singular value thresholding mode, either 'linear' or 'sqrt'. | `'linear'` |\n        \"\"\"\n        assert get_backend_name(us) in [\"numpy\", \"cupy\", \"torch\"]\n        assert get_backend_name(xs) in [\"numpy\", \"cupy\", \"torch\"]\n        assert get_backend_name(us) == get_backend_name(xs)\n        assert us.ndim &gt;= 2 and xs.ndim &gt;= 2, \"us and xs should be multidimensional.\"\n        assert axis1 != axis2, \"axis1 and axis2 should be different\"\n        assert us.shape[axis1] == xs.shape[axis1], \"should share the same time steps.\"\n        assert us.shape[axis2] == 1, (\n            f\"us should be univariate (`us.shape[{axis2}] = 1`) but us's shape is {us.shape}.\"\n        )\n        assert hasattr(polynomial, poly_name), f\"polynomial class named {poly_name} not found.\"\n        self.axis1, self.axis2 = axis1, axis2\n        self.backend = import_backend(us)\n        self.us = self.backend.moveaxis(us, [self.axis1, self.axis2], [-2, -1])\n        xs = self.backend.moveaxis(xs, [self.axis1, self.axis2], [-2, -1])\n        if poly_param is None:\n            poly_param = {}\n        self.poly_cls = functools.partial(\n            getattr(polynomial, poly_name), axis=-2, **poly_param\n        )  # NOTE: GramSchmidt requires axis to be normalized\n        self.poly = self.poly_cls(self.us)\n        self.regressor = BatchRegressor(xs, **regressor_kws)\n        length = self.us.shape[-2]\n        rnd = np.random.default_rng(surrogate_seed)\n        pbar = tqdm(\n            rnd.spawn(surrogate_num), total=surrogate_num, disable=not config.SHOW_PROGRESS_BAR\n        )\n        pbar.set_description(\"random_seq\")\n        self.perms = np.zeros((surrogate_num, length), dtype=np.int32)\n        for idx, rnd in enumerate(pbar):\n            self.perms[idx] = make_permutation(length, rnd=rnd, tolist=False)\n        self.perms = self.to_backend(self.perms)\n        self.results = dict()\n        if self.backend_name == \"torch\":\n            # Pytorch wrapper\n            if not hasattr(self.backend, \"concatenate\"):\n                self.backend.concatenate = self.backend.concat\n\n    @property\n    def rank(self):\n        return self.to_numpy(self.regressor.rank)\n\n    def calc(\n        self,\n        degree_sum: int | tuple[int, ...] | list[int | tuple[int, ...]] = 1,\n        delay_max: int | list[int] = 100,\n        zero_offset: bool | int = True,\n        method: int | tuple | None = None,\n    ):\n        \"\"\"\n        Calculate the IPCs for the given degree sums and delay ranges.\n\n        Parameters:\n            degree_sum (int | tuple | list, optional): Degree sums to compute.\n            delay_max (int | list, optional): Delay ranges for the degree sums.\n            zero_offset (bool | int, optional): Include zero delay or specify an offset.\n            method (int | tuple | None, optional): Filter method (refer to `filter_by` method).\n\n        Notes:\n            The following table explains how `degree_sum` and `delay_max` parameters work together:\n\n            | `degree_sum` | `delay_max` | Description |\n            |-|-|-|\n            | `int` | `int` | Calculate IPCs for all degree tuples with the specified degree sum and delay range. |\n            | `list` | `list` | Calculate IPCs for each degree sum with corresponding delay ranges from the lists. |\n            | Other combinations | - | Raises an assertion error. |\n\n        Examples:\n            ```python\n            profiler.calc(1, 5)  # (1,) with delays 0 to 4.\n            profiler.calc(2, 6, zero_offset=False)  # (1, 1), (2,) with delays 1 to 6.\n            profiler.calc(2, 6, zero_offset=2) # (1, 1), (2,) with delays 2 to 7.\n            profiler.calc([2, 3], [7, 8])  # (1, 1), (2,) with delays 0 to 6; (1, 1, 1), (2, 1), (3,) with delays 0 to 7.\n            profiler.calc(4, 9, method=lambda key: max(key) &gt;= 2)  # (4,), (3, 1), (2, 2), (2, 1, 1) with delays 0 to 8.\n            profiler.calc(5, 10, method=-2)  # (4, 1), (3, 2) with delays 0 to 9.\n            ```\n        \"\"\"\n        assert type(degree_sum) in [int, tuple, list], \"degree_sum should be int, tuple, or list.\"\n        assert type(delay_max) in [int, list], \"delay_max should be int or list.\"\n\n        if type(degree_sum) is int:  # Sum of degrees to evaluate.\n            degree_sum = [degree_sum]\n        if type(delay_max) is int:  # Range of maximum delays.\n            delay_max = [delay_max] * len(degree_sum)\n        assert len(degree_sum) == len(delay_max)\n\n        all_degree_tuples, all_delay_ranges = [], []\n        for degree, delay in zip(degree_sum, delay_max, strict=True):\n            degree_tuples = make_degree_list(degree)\n            if method is not None:\n                degree_tuples = list(filter(self.filter_by(method), degree_tuples))\n            all_degree_tuples += degree_tuples\n            if type(zero_offset) is int:\n                all_delay_ranges += [range(zero_offset, zero_offset + delay)] * len(degree_tuples)\n            elif zero_offset:\n                all_delay_ranges += [range(0, delay)] * len(degree_tuples)\n            else:\n                all_delay_ranges += [range(1, delay + 1)] * len(degree_tuples)\n\n        # Calculate the number of iteration (# of regressor call).\n        total_length = [\n            multi_combination_length(len(delay_range), *Counter(degree_tuple).values())\n            for degree_tuple, delay_range in zip(all_degree_tuples, all_delay_ranges, strict=True)\n        ]\n        total_length = [v for v in total_length if v &gt; 0]\n        total_length = sum(total_length) + len(self.perms) * len(total_length)\n\n        # Main loops.\n        pbar = tqdm(total=total_length, disable=not config.SHOW_PROGRESS_BAR)\n        for degree_tuple, delay_range in zip(all_degree_tuples, all_delay_ranges, strict=True):\n            pbar.set_description(f\"{truncate_tuple(degree_tuple)}\")\n            delay_list = make_delay_list(delay_range, degree_tuple)\n            if len(delay_list) == 0:\n                continue\n            # Surrogate IPCs.\n            ipc_surr = None\n            delays = list(range(1, len(degree_tuple) + 1))\n            for idx, perm in enumerate(self.perms):\n                out = self.regressor(\n                    self.poly, degree_tuple, delays, formatter=(..., perm, slice(None))\n                )\n                if ipc_surr is None:\n                    ipc_surr = zeros_like(self.us, (len(self.perms), *out.shape))\n                ipc_surr[idx] = out\n                pbar.update(1)\n            # Base IPCs.\n            ipc_base = None\n            for idx, delays in enumerate(delay_list):\n                out = self.regressor(self.poly, degree_tuple, delays)\n                if ipc_base is None:\n                    ipc_base = zeros_like(self.us, (len(delay_list), *out.shape))\n                ipc_base[idx] = out\n                pbar.update(1)\n            self.results[degree_tuple] = (delay_list, ipc_base, ipc_surr)\n        pbar.close()\n</code></pre>"},{"location":"profiler/#ipc_module.profiler.UnivariateProfiler.__init__","title":"<code>__init__(us, xs, poly_name='GramSchmidt', poly_param=None, surrogate_num=1000, surrogate_seed=0, axis1=0, axis2=-1, **regressor_kws)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>us</code> <code>Any</code> <p>Input time series. Must be at least 2D and univariate (shape: [t, ..., 1]).</p> required <code>xs</code> <code>Any</code> <p>Output time series. Must be at least 2D (shape: [t, ..., N]).</p> required <code>poly_name</code> <code>str</code> <p>Polynomial class name to use, selectable from the <code>polynomial</code> module.</p> <code>'GramSchmidt'</code> <code>poly_param</code> <code>dict | None</code> <p>Parameters for the polynomial class.</p> <code>None</code> <code>surrogate_num</code> <code>int</code> <p>Number of surrogate datasets to generate.</p> <code>1000</code> <code>surrogate_seed</code> <code>int</code> <p>Random seed for generating surrogate data.</p> <code>0</code> <code>axis1</code> <code>int</code> <p>Axis for time steps; <code>us</code> and <code>xs</code> must match along this axis.</p> <code>0</code> <code>axis2</code> <code>int</code> <p>Axis for variables; defaults to -1 (last axis) for both <code>us</code> and <code>xs</code>.</p> <code>-1</code> <code>regressor_kws</code> <code>dict</code> <p>Additional arguments for the <code>BatchRegressor</code> class.</p> <code>{}</code> Notes <p>Currently supported backends are NumPy, CuPy, and PyTorch. The following conditions must be met for <code>us</code> and <code>xs</code>:</p> <ul> <li>Both must use the same backend.</li> <li>Both must be at least 2D arrays.</li> <li><code>axis1</code> and <code>axis2</code> must be different.</li> <li>They must have the same size along <code>axis1</code> (time steps).</li> <li><code>us</code> must be univariate along <code>axis2</code> (i.e., size 1).</li> </ul> <p><code>GramSchmidt</code> is recommended as the default polynomial class. If you know the specific distribution of <code>us</code>, other polynomial classes may perform better. See the table below for the correspondence between distributions and polynomial classes (Reference: D. Xiu et al. 2002):</p> Distribution of <code>us</code> Polynomial Support Normal (Gaussian) <code>Hermite</code> (-\u221e, \u221e) Gamma <code>Laguerre</code> [0, \u221e) Beta <code>Jacobi</code> [0, 1] Uniform <code>Legendre</code> [-1, 1] Binomial <code>Krawtchouk</code> {0, 1, ..., n} <p><code>BatchRegressor</code> is used internally to compute IPCs in batches, allowing efficient processing of large datasets. The keyword arguments for <code>BatchRegressor</code> can be passed via <code>**regressor_kws</code>.</p> Option Description Default <code>offset</code> The offset applied to the time series data. <code>1000</code> <code>debias</code> If <code>True</code>, removes the mean from the data. <code>True</code> <code>normalize</code> If <code>True</code>, scales the data to unit variance. <code>False</code> <code>threshold_mode</code> Determines the singular value thresholding mode, either 'linear' or 'sqrt'. <code>'linear'</code> Source code in <code>src/ipc_module/profiler.py</code> <pre><code>def __init__(\n    self,\n    us,\n    xs,\n    poly_name: str = \"GramSchmidt\",\n    poly_param: dict | None = None,\n    surrogate_num: int = 1000,\n    surrogate_seed: int = 0,\n    axis1: int = 0,\n    axis2: int = -1,\n    **regressor_kws,\n):\n    \"\"\"\n    Parameters:\n        us (Any): Input time series. Must be at least 2D and univariate (shape: [t, ..., 1]).\n        xs (Any): Output time series. Must be at least 2D (shape: [t, ..., N]).\n        poly_name (str, optional): Polynomial class name to use, selectable from the `polynomial` module.\n        poly_param (dict | None, optional): Parameters for the polynomial class.\n        surrogate_num (int, optional): Number of surrogate datasets to generate.\n        surrogate_seed (int, optional): Random seed for generating surrogate data.\n        axis1 (int, optional): Axis for time steps; `us` and `xs` must match along this axis.\n        axis2 (int, optional): Axis for variables; defaults to -1 (last axis) for both `us` and `xs`.\n        regressor_kws (dict): Additional arguments for the `BatchRegressor` class.\n\n    Notes:\n        Currently supported backends are NumPy, CuPy, and PyTorch.\n        The following conditions must be met for `us` and `xs`:\n\n        - Both must use the same backend.\n        - Both must be at least 2D arrays.\n        - `axis1` and `axis2` must be different.\n        - They must have the same size along `axis1` (time steps).\n        - `us` must be univariate along `axis2` (i.e., size 1).\n\n        `GramSchmidt` is recommended as the default polynomial class.\n        If you know the specific distribution of `us`, other polynomial classes may perform better.\n        See the table below for the correspondence between distributions and polynomial classes\n        (Reference: [D. Xiu et al. 2002](https://epubs.siam.org/doi/10.1137/S1064827501387826)):\n\n        | Distribution of `us` | Polynomial | Support |\n        |-|-|-|\n        | Normal (Gaussian) | `Hermite` | (-\u221e, \u221e) |\n        | Gamma | `Laguerre` | [0, \u221e) |\n        | Beta | `Jacobi` | [0, 1] |\n        | Uniform | `Legendre` | [-1, 1] |\n        | Binomial | `Krawtchouk` | {0, 1, ..., n} |\n\n        `BatchRegressor` is used internally to compute IPCs in batches, allowing efficient processing of large datasets.\n        The keyword arguments for `BatchRegressor` can be passed via `**regressor_kws`.\n\n        | Option | Description | Default |\n        |-|-|-|\n        | `offset` | The offset applied to the time series data. | `1000` |\n        | `debias` | If `True`, removes the mean from the data. | `True` |\n        | `normalize` | If `True`, scales the data to unit variance. | `False` |\n        | `threshold_mode` | Determines the singular value thresholding mode, either 'linear' or 'sqrt'. | `'linear'` |\n    \"\"\"\n    assert get_backend_name(us) in [\"numpy\", \"cupy\", \"torch\"]\n    assert get_backend_name(xs) in [\"numpy\", \"cupy\", \"torch\"]\n    assert get_backend_name(us) == get_backend_name(xs)\n    assert us.ndim &gt;= 2 and xs.ndim &gt;= 2, \"us and xs should be multidimensional.\"\n    assert axis1 != axis2, \"axis1 and axis2 should be different\"\n    assert us.shape[axis1] == xs.shape[axis1], \"should share the same time steps.\"\n    assert us.shape[axis2] == 1, (\n        f\"us should be univariate (`us.shape[{axis2}] = 1`) but us's shape is {us.shape}.\"\n    )\n    assert hasattr(polynomial, poly_name), f\"polynomial class named {poly_name} not found.\"\n    self.axis1, self.axis2 = axis1, axis2\n    self.backend = import_backend(us)\n    self.us = self.backend.moveaxis(us, [self.axis1, self.axis2], [-2, -1])\n    xs = self.backend.moveaxis(xs, [self.axis1, self.axis2], [-2, -1])\n    if poly_param is None:\n        poly_param = {}\n    self.poly_cls = functools.partial(\n        getattr(polynomial, poly_name), axis=-2, **poly_param\n    )  # NOTE: GramSchmidt requires axis to be normalized\n    self.poly = self.poly_cls(self.us)\n    self.regressor = BatchRegressor(xs, **regressor_kws)\n    length = self.us.shape[-2]\n    rnd = np.random.default_rng(surrogate_seed)\n    pbar = tqdm(\n        rnd.spawn(surrogate_num), total=surrogate_num, disable=not config.SHOW_PROGRESS_BAR\n    )\n    pbar.set_description(\"random_seq\")\n    self.perms = np.zeros((surrogate_num, length), dtype=np.int32)\n    for idx, rnd in enumerate(pbar):\n        self.perms[idx] = make_permutation(length, rnd=rnd, tolist=False)\n    self.perms = self.to_backend(self.perms)\n    self.results = dict()\n    if self.backend_name == \"torch\":\n        # Pytorch wrapper\n        if not hasattr(self.backend, \"concatenate\"):\n            self.backend.concatenate = self.backend.concat\n</code></pre>"},{"location":"profiler/#ipc_module.profiler.UnivariateProfiler.calc","title":"<code>calc(degree_sum=1, delay_max=100, zero_offset=True, method=None)</code>","text":"<p>Calculate the IPCs for the given degree sums and delay ranges.</p> <p>Parameters:</p> Name Type Description Default <code>degree_sum</code> <code>int | tuple | list</code> <p>Degree sums to compute.</p> <code>1</code> <code>delay_max</code> <code>int | list</code> <p>Delay ranges for the degree sums.</p> <code>100</code> <code>zero_offset</code> <code>bool | int</code> <p>Include zero delay or specify an offset.</p> <code>True</code> <code>method</code> <code>int | tuple | None</code> <p>Filter method (refer to <code>filter_by</code> method).</p> <code>None</code> Notes <p>The following table explains how <code>degree_sum</code> and <code>delay_max</code> parameters work together:</p> <code>degree_sum</code> <code>delay_max</code> Description <code>int</code> <code>int</code> Calculate IPCs for all degree tuples with the specified degree sum and delay range. <code>list</code> <code>list</code> Calculate IPCs for each degree sum with corresponding delay ranges from the lists. Other combinations - Raises an assertion error. <p>Examples:</p> <pre><code>profiler.calc(1, 5)  # (1,) with delays 0 to 4.\nprofiler.calc(2, 6, zero_offset=False)  # (1, 1), (2,) with delays 1 to 6.\nprofiler.calc(2, 6, zero_offset=2) # (1, 1), (2,) with delays 2 to 7.\nprofiler.calc([2, 3], [7, 8])  # (1, 1), (2,) with delays 0 to 6; (1, 1, 1), (2, 1), (3,) with delays 0 to 7.\nprofiler.calc(4, 9, method=lambda key: max(key) &gt;= 2)  # (4,), (3, 1), (2, 2), (2, 1, 1) with delays 0 to 8.\nprofiler.calc(5, 10, method=-2)  # (4, 1), (3, 2) with delays 0 to 9.\n</code></pre> Source code in <code>src/ipc_module/profiler.py</code> <pre><code>def calc(\n    self,\n    degree_sum: int | tuple[int, ...] | list[int | tuple[int, ...]] = 1,\n    delay_max: int | list[int] = 100,\n    zero_offset: bool | int = True,\n    method: int | tuple | None = None,\n):\n    \"\"\"\n    Calculate the IPCs for the given degree sums and delay ranges.\n\n    Parameters:\n        degree_sum (int | tuple | list, optional): Degree sums to compute.\n        delay_max (int | list, optional): Delay ranges for the degree sums.\n        zero_offset (bool | int, optional): Include zero delay or specify an offset.\n        method (int | tuple | None, optional): Filter method (refer to `filter_by` method).\n\n    Notes:\n        The following table explains how `degree_sum` and `delay_max` parameters work together:\n\n        | `degree_sum` | `delay_max` | Description |\n        |-|-|-|\n        | `int` | `int` | Calculate IPCs for all degree tuples with the specified degree sum and delay range. |\n        | `list` | `list` | Calculate IPCs for each degree sum with corresponding delay ranges from the lists. |\n        | Other combinations | - | Raises an assertion error. |\n\n    Examples:\n        ```python\n        profiler.calc(1, 5)  # (1,) with delays 0 to 4.\n        profiler.calc(2, 6, zero_offset=False)  # (1, 1), (2,) with delays 1 to 6.\n        profiler.calc(2, 6, zero_offset=2) # (1, 1), (2,) with delays 2 to 7.\n        profiler.calc([2, 3], [7, 8])  # (1, 1), (2,) with delays 0 to 6; (1, 1, 1), (2, 1), (3,) with delays 0 to 7.\n        profiler.calc(4, 9, method=lambda key: max(key) &gt;= 2)  # (4,), (3, 1), (2, 2), (2, 1, 1) with delays 0 to 8.\n        profiler.calc(5, 10, method=-2)  # (4, 1), (3, 2) with delays 0 to 9.\n        ```\n    \"\"\"\n    assert type(degree_sum) in [int, tuple, list], \"degree_sum should be int, tuple, or list.\"\n    assert type(delay_max) in [int, list], \"delay_max should be int or list.\"\n\n    if type(degree_sum) is int:  # Sum of degrees to evaluate.\n        degree_sum = [degree_sum]\n    if type(delay_max) is int:  # Range of maximum delays.\n        delay_max = [delay_max] * len(degree_sum)\n    assert len(degree_sum) == len(delay_max)\n\n    all_degree_tuples, all_delay_ranges = [], []\n    for degree, delay in zip(degree_sum, delay_max, strict=True):\n        degree_tuples = make_degree_list(degree)\n        if method is not None:\n            degree_tuples = list(filter(self.filter_by(method), degree_tuples))\n        all_degree_tuples += degree_tuples\n        if type(zero_offset) is int:\n            all_delay_ranges += [range(zero_offset, zero_offset + delay)] * len(degree_tuples)\n        elif zero_offset:\n            all_delay_ranges += [range(0, delay)] * len(degree_tuples)\n        else:\n            all_delay_ranges += [range(1, delay + 1)] * len(degree_tuples)\n\n    # Calculate the number of iteration (# of regressor call).\n    total_length = [\n        multi_combination_length(len(delay_range), *Counter(degree_tuple).values())\n        for degree_tuple, delay_range in zip(all_degree_tuples, all_delay_ranges, strict=True)\n    ]\n    total_length = [v for v in total_length if v &gt; 0]\n    total_length = sum(total_length) + len(self.perms) * len(total_length)\n\n    # Main loops.\n    pbar = tqdm(total=total_length, disable=not config.SHOW_PROGRESS_BAR)\n    for degree_tuple, delay_range in zip(all_degree_tuples, all_delay_ranges, strict=True):\n        pbar.set_description(f\"{truncate_tuple(degree_tuple)}\")\n        delay_list = make_delay_list(delay_range, degree_tuple)\n        if len(delay_list) == 0:\n            continue\n        # Surrogate IPCs.\n        ipc_surr = None\n        delays = list(range(1, len(degree_tuple) + 1))\n        for idx, perm in enumerate(self.perms):\n            out = self.regressor(\n                self.poly, degree_tuple, delays, formatter=(..., perm, slice(None))\n            )\n            if ipc_surr is None:\n                ipc_surr = zeros_like(self.us, (len(self.perms), *out.shape))\n            ipc_surr[idx] = out\n            pbar.update(1)\n        # Base IPCs.\n        ipc_base = None\n        for idx, delays in enumerate(delay_list):\n            out = self.regressor(self.poly, degree_tuple, delays)\n            if ipc_base is None:\n                ipc_base = zeros_like(self.us, (len(delay_list), *out.shape))\n            ipc_base[idx] = out\n            pbar.update(1)\n        self.results[degree_tuple] = (delay_list, ipc_base, ipc_surr)\n    pbar.close()\n</code></pre>"},{"location":"profiler/#ipc_module.profiler.UnivariateViewer","title":"<code>UnivariateViewer</code>","text":"<p>               Bases: <code>object</code></p> <p>Viewer class to load and visualize profiling results. This class does not have the original time series (i.e., <code>us</code> and <code>xs</code>), so it cannot calculate IPC.</p> Source code in <code>src/ipc_module/profiler.py</code> <pre><code>class UnivariateViewer(object):\n    \"\"\"\n    Viewer class to load and visualize profiling results.\n    This class does not have the original time series (i.e., `us` and `xs`), so it cannot calculate IPC.\n    \"\"\"\n\n    def __init__(self, path: str, method: int | tuple | None = None):\n        \"\"\"\n        Parameters:\n            path (str): File path to load results.\n            method (int | tuple | None, optional): Filter method (see `filter_by` method).\n\n        Notes:\n            You can partially load results by specifying the `method` argument.\n\n        Examples:\n            ```python\n            viewer = UnivariateViewer(\"path/to/results.npz\")  # Load all results\n            viewer = UnivariateViewer(\"path/to/results.npz\", method=3)  # Load only degree sums of 3\n            viewer = UnivariateViewer(\"path/to/results.npz\", method=(2, 1))  # Load only degree tuple of (2, 1)\n            ```\n        \"\"\"\n        self.backend = np\n        if path is None:\n            self.results = {}\n            self.rank = []\n            self.info = {}\n        else:\n            self.load(path, method=method)\n\n    def __getitem__(self, degree_tuple: tuple):\n        \"\"\"\n        Retrieves profiling results for a given degree tuple.\n\n        Parameters:\n            degree_tuple (tuple): Degree tuple to fetch results for (e.g., (1, 2), (3,), etc.).\n\n        Returns:\n            delay (list): Evaluated delays (e.g., [(0,), (1,), ...]), with length matching the number of delays.\n            ipc_base (np.ndarray): IPCs for original data, shaped as [# of delays, *batch_shape, 1].\n            ipc_surr (np.ndarray): IPCs for surrogate data, shaped as [# of surrogates, *batch_shape, 1].\n\n        Examples:\n            ```python\n            delay, ipc_base, ipc_surr = viewer[(2, 1)]  # Fetch results for degree tuple (2, 1)\n            ```\n        \"\"\"\n        assert type(degree_tuple) is tuple\n        if degree_tuple not in self.results:\n            return None\n        delay, base, surr = self.results[degree_tuple]\n        return delay, self.to_numpy(base), self.to_numpy(surr)\n\n    def __len__(self):\n        return len(self.results)\n\n    @property\n    def backend_name(self):\n        return self.backend.__name__\n\n    @staticmethod\n    def filter_by(method: int | tuple | None = None):\n        \"\"\"\n        Creates a filter function based on the specified method.\n\n        Parameters:\n            method (int | tuple | None, optional): Method to filter by. Either an integer, a tuple of degrees, or None.\n\n        Returns:\n            filter_func (callable | None): A function that takes a degree tuple as input and returns True if it matches the filter criteria, or None if no filtering is applied.\n\n        Notes:\n            You might not directly use this method; a function with `method` argument in other methods calls this internally.\n            The filter function works as follows based on the `method` type:\n\n            |Type of `method`|Description|\n            |-|-|\n            | `int` (positive) | Extract degree tuples where the sum of degrees equals the specified value (e.g., `3` extracts with degree sums of 3, such as `(1, 2)` and `(3,)`). |\n            | `int` (negative) | Extract degree tuples where the length of degree tuple equals the specified value (e.g., `-3` extracts with degree lengths of 3, such as `(1, 1, 1)` and `(2, 2, 3)`). |\n            | `tuple` | Extract only degree tuples that match the specified tuple of degrees. |\n            | `None` | No filtering, i.e., extract all degree tuples (default). |\n        \"\"\"\n        if type(method) is int:\n            if method &gt; 0:\n                return lambda key: sum(key) == method\n            elif method &lt; 0:\n                return lambda key: len(key) == -method\n            else:\n                raise ValueError(\"filter method should be non-zero value.\")\n        elif type(method) is tuple:\n            return lambda key: key == method\n        else:\n            return method\n\n    @staticmethod\n    def to_numpy(val):\n        name = val.__class__.__module__\n        if name == \"torch\":\n            return val.detach().cpu().numpy()\n        elif name == \"cupy\":\n            return import_backend(val).asnumpy(val)\n        else:\n            return val\n\n    def to_backend(self, val: np.ndarray):\n        name = self.backend_name\n        if name == \"torch\":\n            return self.backend.from_numpy(val).to(self.us.device)\n        elif name == \"cupy\":\n            return self.backend.asarray(val)\n        else:\n            return val\n\n    def keys(self, method: int | tuple | None = None):\n        if method is None:\n            return self.results.keys()\n        else:\n            return filter(self.filter_by(method), self.results.keys())\n\n    def items(self, method: int | tuple | None = None):\n        return map(lambda key: (key, self[key]), self.keys(method))\n\n    def values(self, method: int | tuple | None = None):\n        return map(lambda key: self[key], self.keys(method))\n\n    def calc_surr_max(self, key=None, surr=None, max_scale=1.0):\n        if surr is None:\n            surr = self.to_numpy(self.results[key][2])\n        surr = self.to_numpy(surr)\n        surr_max = np.max(surr, axis=0)\n        return surr_max * max_scale\n\n    def to_dataframe(\n        self,\n        method: int | tuple | None = None,\n        extracted_batch: tuple = (...,),\n        squeeze: bool = False,\n        truncate_by_rank: bool = True,\n        leave: bool = False,\n        max_scale: float = 1.0,\n    ):\n        \"\"\"\n        Converts the profiling results to a polars DataFrame for easier analysis and visualization.\n\n        Parameters:\n            method (int | tuple | None, optional): Filter method (see `filter_by` method).\n            extracted_batch (tuple, optional): Batch indices to extract from the results.\n            squeeze (bool, optional): Whether to squeeze singleton dimensions in the IPCs.\n            truncate_by_rank (bool, optional): Whether to truncate IPCs by rank.\n            leave (bool, optional): Whether to leave the progress bar after completion.\n            max_scale (float, optional): Surrogate max scaling factor for IPC thresholding.\n\n        Returns:\n            df (pl.DataFrame): `polars.DataFrame` containing the profiling results.\n            rank (np.ndarray): Rank array. The shape is equal to the batch shape.\n\n        Examples:\n            ```python\n            df, rank = viewer.to_dataframe(method=3, squeeze=True)  # Convert results with degree sums of 3 to DataFrame\n            df, rank = viewer.to_dataframe(method=(2, 1), truncate_by_rank=False)  # Convert results for degree tuple (2, 1) without truncation\n            df, rank = viewer.to_dataframe(max_scale=2.0)  # Convert all results with surrogate max scaled by 2.0\n            ```\n        \"\"\"\n        keys = list(self.keys(method))\n        batch = (slice(None), *extracted_batch, 0)\n        max_length = max([len(key) for key in keys]) if len(keys) &gt; 0 else 0\n        degrees, components, delays, ipcs = [], [], [], []\n        for degree_tuple, (delay, base, surr) in tqdm(\n            self.items(method), total=len(keys), leave=leave, disable=not config.SHOW_PROGRESS_BAR\n        ):\n            degrees.append(np.full(len(delay), sum(degree_tuple), dtype=np.int32))\n            c_mat = np.full((len(delay), max_length), -1, dtype=np.int32)\n            c_mat[:, : len(degree_tuple)] = degree_tuple\n            components.append(c_mat)\n            d_mat = np.full((len(delay), max_length), -1, dtype=np.int32)\n            d_mat[:, : len(degree_tuple)] = delay\n            delays.append(d_mat)\n            ipc = base[batch] * (\n                base[batch]\n                &gt; self.calc_surr_max(surr=surr[batch], key=degree_tuple, max_scale=max_scale)\n            )\n            ipcs.append(ipc)\n        rank = self.rank\n        if max_length == 0:\n            degrees = np.empty((0,), dtype=np.int32)\n            components = np.empty((0,), dtype=np.int32)\n            delays = np.empty((0,), dtype=np.int32)\n            ipcs = np.empty((0, *rank.shape), dtype=np.float64)\n        else:\n            degrees = np.concatenate(degrees, axis=0)\n            components = np.concatenate(components, axis=0)\n            delays = np.concatenate(delays, axis=0)\n            ipcs = np.concatenate(ipcs, axis=0)\n        if squeeze:\n            batch_shape = [dim for dim in ipcs.shape[1:] if dim != 1]\n            ipcs = ipcs.reshape(-1, *batch_shape)\n            rank = rank.reshape(*batch_shape)\n        df = pl.DataFrame(\n            {\n                \"degree\": np.array(degrees),\n                **{f\"cmp_{idx}\": components[:, idx] for idx in range(max_length)},\n                **{f\"del_{idx}\": delays[:, idx] for idx in range(max_length)},\n                **{\n                    f\"ipc_{'_'.join(map(str, idx))}\": ipcs[(slice(None), *idx)]\n                    for idx in np.ndindex(*ipcs.shape[1:])\n                },\n            }\n        )\n        if truncate_by_rank:\n            for idx in np.ndindex(*ipcs.shape[1:]):\n                col = f\"ipc_{'_'.join(map(str, idx))}\"\n                df = df.sort(pl.col(col), descending=True)\n                df = df.with_columns(pl.col(col).cum_sum().name.prefix(\"acc_\"))\n                df = df.with_columns(\n                    pl.when(pl.col(f\"acc_{col}\") &gt; rank[(*idx,)])\n                    .then(0.0)\n                    .otherwise(pl.col(col))\n                    .alias(col)\n                )\n            df = df.select(~cs.starts_with(\"acc_\")).sort(pl.col(\"degree\"))\n        return df, rank\n\n    def total(self, method: int | tuple | None = None, max_scale: float = 1.0):\n        \"\"\"\n        Calculates the total IPCs over all degree tuples stored in the results,\n        optionally filtered by a specified method.\n\n        Parameters:\n            method (int | tuple | None, optional): Filter method (see `filter_by` method).\n            max_scale (float, optional): Surrogate max scaling factor for IPC thresholding.\n\n        Returns:\n            total_ipc (np.ndarray | None): Total IPCs over all degree tuples. The shape is equal to the batch shape.\n\n        Examples:\n            ```python\n            ipc_all = viewer.total()  # Calculate total IPCs for all degree tuples\n            ipc_3 = viewer.total(method=3)  # Calculate total IPCs for degree sums of 3\n            ipc_2_1 = viewer.total(method=(2, 1))  # Calculate total IPCs for degree tuple (2, 1)\n            ```\n        \"\"\"\n        ipcs = []\n        for degree_tuple in self.keys(method):\n            res = self[degree_tuple]\n            if res is None:\n                continue\n            _delay, base, surr = res\n            ipcs.append(\n                base * (base &gt; self.calc_surr_max(surr=surr, key=degree_tuple, max_scale=max_scale))\n            )\n        if len(ipcs) &gt; 0:\n            return np.concatenate(ipcs, axis=0).sum(axis=0)[..., 0]\n\n    def save(self, path: str, **kwargs):\n        \"\"\"\n        Saves the profiling results to a specified file path in either `.npz` or joblib format.\n\n        Parameters:\n            path (str): File path to save the results.\n            kwargs (dict): Additional information to save alongside the results.\n\n        Notes:\n            If the extension is `.npz`, the results are saved in compressed format using [`numpy.savez_compressed`](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html).\n            Otherwise, they are saved using [`joblib.dump`](https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html).\n\n            `kwargs` can be used to store any additional metadata or information you want to associate with the profiling results.\n        \"\"\"\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        if self.backend_name == \"torch\":\n            self.backend.cuda.synchronize(self.us.device)\n        if path.endswith(\".npz\"):\n            with open(path, mode=\"wb\") as f:\n                np.savez_compressed(\n                    f,\n                    **{\n                        \"_\".join(map(str, degree_tuple)) + f\"/{key}\": np.asarray(val)\n                        for degree_tuple, (delay, base, surr) in self.items()\n                        for key, val in dict(delay=delay, base=base, surr=surr).items()\n                    },\n                    **{f\"kwargs/{key}\": value for key, value in kwargs.items()},\n                    rank=self.rank,\n                )\n        else:\n            with open(path, mode=\"wb\") as f:\n                joblib.dump(\n                    [{key: value for key, value in self.items()}, self.rank, kwargs],\n                    f,\n                    compress=True,\n                )\n\n    def load(self, path: str, method: int | tuple | None = None):\n        if path.endswith(\".npz\"):\n            with open(path, mode=\"rb\") as f:\n                npz = np.load(f)\n                keys = sorted(\n                    [\n                        tuple(map(int, degree_str.split(\"_\")))\n                        for degree_str in set(\n                            key.split(\"/\")[0]\n                            for key in npz.keys()\n                            if key != \"rank\" and not key.startswith(\"kwargs\")\n                        )\n                    ],\n                    key=lambda v: (sum(v), *v),\n                )\n                if method is not None:\n                    keys = filter(self.filter_by(method), keys)\n                func = dict(\n                    # delay=lambda v: list(map(tuple, v)),\n                    delay=np.asarray,\n                    base=np.asarray,\n                    surr=np.asarray,\n                )\n                self.results = {\n                    degree: tuple(\n                        func[name](npz[f\"{'_'.join(map(str, degree))}/{name}\"])\n                        for name in [\"delay\", \"base\", \"surr\"]\n                    )\n                    for degree in keys\n                }\n                self.rank = npz[\"rank\"]\n                rest = {\n                    key.split(\"/\")[1]: npz[key] for key in npz.keys() if key.startswith(\"kwargs\")\n                }\n\n        else:\n            with open(path, mode=\"rb\") as f:\n                self._results, self.rank, *rest = joblib.load(f)\n            if len(rest) &gt; 0:\n                rest = rest[0]\n            else:\n                rest = {}\n        self.info = rest\n</code></pre>"},{"location":"profiler/#ipc_module.profiler.UnivariateViewer.__getitem__","title":"<code>__getitem__(degree_tuple)</code>","text":"<p>Retrieves profiling results for a given degree tuple.</p> <p>Parameters:</p> Name Type Description Default <code>degree_tuple</code> <code>tuple</code> <p>Degree tuple to fetch results for (e.g., (1, 2), (3,), etc.).</p> required <p>Returns:</p> Name Type Description <code>delay</code> <code>list</code> <p>Evaluated delays (e.g., [(0,), (1,), ...]), with length matching the number of delays.</p> <code>ipc_base</code> <code>ndarray</code> <p>IPCs for original data, shaped as [# of delays, *batch_shape, 1].</p> <code>ipc_surr</code> <code>ndarray</code> <p>IPCs for surrogate data, shaped as [# of surrogates, *batch_shape, 1].</p> <p>Examples:</p> <pre><code>delay, ipc_base, ipc_surr = viewer[(2, 1)]  # Fetch results for degree tuple (2, 1)\n</code></pre> Source code in <code>src/ipc_module/profiler.py</code> <pre><code>def __getitem__(self, degree_tuple: tuple):\n    \"\"\"\n    Retrieves profiling results for a given degree tuple.\n\n    Parameters:\n        degree_tuple (tuple): Degree tuple to fetch results for (e.g., (1, 2), (3,), etc.).\n\n    Returns:\n        delay (list): Evaluated delays (e.g., [(0,), (1,), ...]), with length matching the number of delays.\n        ipc_base (np.ndarray): IPCs for original data, shaped as [# of delays, *batch_shape, 1].\n        ipc_surr (np.ndarray): IPCs for surrogate data, shaped as [# of surrogates, *batch_shape, 1].\n\n    Examples:\n        ```python\n        delay, ipc_base, ipc_surr = viewer[(2, 1)]  # Fetch results for degree tuple (2, 1)\n        ```\n    \"\"\"\n    assert type(degree_tuple) is tuple\n    if degree_tuple not in self.results:\n        return None\n    delay, base, surr = self.results[degree_tuple]\n    return delay, self.to_numpy(base), self.to_numpy(surr)\n</code></pre>"},{"location":"profiler/#ipc_module.profiler.UnivariateViewer.__init__","title":"<code>__init__(path, method=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path to load results.</p> required <code>method</code> <code>int | tuple | None</code> <p>Filter method (see <code>filter_by</code> method).</p> <code>None</code> Notes <p>You can partially load results by specifying the <code>method</code> argument.</p> <p>Examples:</p> <pre><code>viewer = UnivariateViewer(\"path/to/results.npz\")  # Load all results\nviewer = UnivariateViewer(\"path/to/results.npz\", method=3)  # Load only degree sums of 3\nviewer = UnivariateViewer(\"path/to/results.npz\", method=(2, 1))  # Load only degree tuple of (2, 1)\n</code></pre> Source code in <code>src/ipc_module/profiler.py</code> <pre><code>def __init__(self, path: str, method: int | tuple | None = None):\n    \"\"\"\n    Parameters:\n        path (str): File path to load results.\n        method (int | tuple | None, optional): Filter method (see `filter_by` method).\n\n    Notes:\n        You can partially load results by specifying the `method` argument.\n\n    Examples:\n        ```python\n        viewer = UnivariateViewer(\"path/to/results.npz\")  # Load all results\n        viewer = UnivariateViewer(\"path/to/results.npz\", method=3)  # Load only degree sums of 3\n        viewer = UnivariateViewer(\"path/to/results.npz\", method=(2, 1))  # Load only degree tuple of (2, 1)\n        ```\n    \"\"\"\n    self.backend = np\n    if path is None:\n        self.results = {}\n        self.rank = []\n        self.info = {}\n    else:\n        self.load(path, method=method)\n</code></pre>"},{"location":"profiler/#ipc_module.profiler.UnivariateViewer.filter_by","title":"<code>filter_by(method=None)</code>  <code>staticmethod</code>","text":"<p>Creates a filter function based on the specified method.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>int | tuple | None</code> <p>Method to filter by. Either an integer, a tuple of degrees, or None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>filter_func</code> <code>callable | None</code> <p>A function that takes a degree tuple as input and returns True if it matches the filter criteria, or None if no filtering is applied.</p> Notes <p>You might not directly use this method; a function with <code>method</code> argument in other methods calls this internally. The filter function works as follows based on the <code>method</code> type:</p> Type of <code>method</code> Description <code>int</code> (positive) Extract degree tuples where the sum of degrees equals the specified value (e.g., <code>3</code> extracts with degree sums of 3, such as <code>(1, 2)</code> and <code>(3,)</code>). <code>int</code> (negative) Extract degree tuples where the length of degree tuple equals the specified value (e.g., <code>-3</code> extracts with degree lengths of 3, such as <code>(1, 1, 1)</code> and <code>(2, 2, 3)</code>). <code>tuple</code> Extract only degree tuples that match the specified tuple of degrees. <code>None</code> No filtering, i.e., extract all degree tuples (default). Source code in <code>src/ipc_module/profiler.py</code> <pre><code>@staticmethod\ndef filter_by(method: int | tuple | None = None):\n    \"\"\"\n    Creates a filter function based on the specified method.\n\n    Parameters:\n        method (int | tuple | None, optional): Method to filter by. Either an integer, a tuple of degrees, or None.\n\n    Returns:\n        filter_func (callable | None): A function that takes a degree tuple as input and returns True if it matches the filter criteria, or None if no filtering is applied.\n\n    Notes:\n        You might not directly use this method; a function with `method` argument in other methods calls this internally.\n        The filter function works as follows based on the `method` type:\n\n        |Type of `method`|Description|\n        |-|-|\n        | `int` (positive) | Extract degree tuples where the sum of degrees equals the specified value (e.g., `3` extracts with degree sums of 3, such as `(1, 2)` and `(3,)`). |\n        | `int` (negative) | Extract degree tuples where the length of degree tuple equals the specified value (e.g., `-3` extracts with degree lengths of 3, such as `(1, 1, 1)` and `(2, 2, 3)`). |\n        | `tuple` | Extract only degree tuples that match the specified tuple of degrees. |\n        | `None` | No filtering, i.e., extract all degree tuples (default). |\n    \"\"\"\n    if type(method) is int:\n        if method &gt; 0:\n            return lambda key: sum(key) == method\n        elif method &lt; 0:\n            return lambda key: len(key) == -method\n        else:\n            raise ValueError(\"filter method should be non-zero value.\")\n    elif type(method) is tuple:\n        return lambda key: key == method\n    else:\n        return method\n</code></pre>"},{"location":"profiler/#ipc_module.profiler.UnivariateViewer.save","title":"<code>save(path, **kwargs)</code>","text":"<p>Saves the profiling results to a specified file path in either <code>.npz</code> or joblib format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path to save the results.</p> required <code>kwargs</code> <code>dict</code> <p>Additional information to save alongside the results.</p> <code>{}</code> Notes <p>If the extension is <code>.npz</code>, the results are saved in compressed format using <code>numpy.savez_compressed</code>. Otherwise, they are saved using <code>joblib.dump</code>.</p> <p><code>kwargs</code> can be used to store any additional metadata or information you want to associate with the profiling results.</p> Source code in <code>src/ipc_module/profiler.py</code> <pre><code>def save(self, path: str, **kwargs):\n    \"\"\"\n    Saves the profiling results to a specified file path in either `.npz` or joblib format.\n\n    Parameters:\n        path (str): File path to save the results.\n        kwargs (dict): Additional information to save alongside the results.\n\n    Notes:\n        If the extension is `.npz`, the results are saved in compressed format using [`numpy.savez_compressed`](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html).\n        Otherwise, they are saved using [`joblib.dump`](https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html).\n\n        `kwargs` can be used to store any additional metadata or information you want to associate with the profiling results.\n    \"\"\"\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    if self.backend_name == \"torch\":\n        self.backend.cuda.synchronize(self.us.device)\n    if path.endswith(\".npz\"):\n        with open(path, mode=\"wb\") as f:\n            np.savez_compressed(\n                f,\n                **{\n                    \"_\".join(map(str, degree_tuple)) + f\"/{key}\": np.asarray(val)\n                    for degree_tuple, (delay, base, surr) in self.items()\n                    for key, val in dict(delay=delay, base=base, surr=surr).items()\n                },\n                **{f\"kwargs/{key}\": value for key, value in kwargs.items()},\n                rank=self.rank,\n            )\n    else:\n        with open(path, mode=\"wb\") as f:\n            joblib.dump(\n                [{key: value for key, value in self.items()}, self.rank, kwargs],\n                f,\n                compress=True,\n            )\n</code></pre>"},{"location":"profiler/#ipc_module.profiler.UnivariateViewer.to_dataframe","title":"<code>to_dataframe(method=None, extracted_batch=(...,), squeeze=False, truncate_by_rank=True, leave=False, max_scale=1.0)</code>","text":"<p>Converts the profiling results to a polars DataFrame for easier analysis and visualization.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>int | tuple | None</code> <p>Filter method (see <code>filter_by</code> method).</p> <code>None</code> <code>extracted_batch</code> <code>tuple</code> <p>Batch indices to extract from the results.</p> <code>(...,)</code> <code>squeeze</code> <code>bool</code> <p>Whether to squeeze singleton dimensions in the IPCs.</p> <code>False</code> <code>truncate_by_rank</code> <code>bool</code> <p>Whether to truncate IPCs by rank.</p> <code>True</code> <code>leave</code> <code>bool</code> <p>Whether to leave the progress bar after completion.</p> <code>False</code> <code>max_scale</code> <code>float</code> <p>Surrogate max scaling factor for IPC thresholding.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p><code>polars.DataFrame</code> containing the profiling results.</p> <code>rank</code> <code>ndarray</code> <p>Rank array. The shape is equal to the batch shape.</p> <p>Examples:</p> <pre><code>df, rank = viewer.to_dataframe(method=3, squeeze=True)  # Convert results with degree sums of 3 to DataFrame\ndf, rank = viewer.to_dataframe(method=(2, 1), truncate_by_rank=False)  # Convert results for degree tuple (2, 1) without truncation\ndf, rank = viewer.to_dataframe(max_scale=2.0)  # Convert all results with surrogate max scaled by 2.0\n</code></pre> Source code in <code>src/ipc_module/profiler.py</code> <pre><code>def to_dataframe(\n    self,\n    method: int | tuple | None = None,\n    extracted_batch: tuple = (...,),\n    squeeze: bool = False,\n    truncate_by_rank: bool = True,\n    leave: bool = False,\n    max_scale: float = 1.0,\n):\n    \"\"\"\n    Converts the profiling results to a polars DataFrame for easier analysis and visualization.\n\n    Parameters:\n        method (int | tuple | None, optional): Filter method (see `filter_by` method).\n        extracted_batch (tuple, optional): Batch indices to extract from the results.\n        squeeze (bool, optional): Whether to squeeze singleton dimensions in the IPCs.\n        truncate_by_rank (bool, optional): Whether to truncate IPCs by rank.\n        leave (bool, optional): Whether to leave the progress bar after completion.\n        max_scale (float, optional): Surrogate max scaling factor for IPC thresholding.\n\n    Returns:\n        df (pl.DataFrame): `polars.DataFrame` containing the profiling results.\n        rank (np.ndarray): Rank array. The shape is equal to the batch shape.\n\n    Examples:\n        ```python\n        df, rank = viewer.to_dataframe(method=3, squeeze=True)  # Convert results with degree sums of 3 to DataFrame\n        df, rank = viewer.to_dataframe(method=(2, 1), truncate_by_rank=False)  # Convert results for degree tuple (2, 1) without truncation\n        df, rank = viewer.to_dataframe(max_scale=2.0)  # Convert all results with surrogate max scaled by 2.0\n        ```\n    \"\"\"\n    keys = list(self.keys(method))\n    batch = (slice(None), *extracted_batch, 0)\n    max_length = max([len(key) for key in keys]) if len(keys) &gt; 0 else 0\n    degrees, components, delays, ipcs = [], [], [], []\n    for degree_tuple, (delay, base, surr) in tqdm(\n        self.items(method), total=len(keys), leave=leave, disable=not config.SHOW_PROGRESS_BAR\n    ):\n        degrees.append(np.full(len(delay), sum(degree_tuple), dtype=np.int32))\n        c_mat = np.full((len(delay), max_length), -1, dtype=np.int32)\n        c_mat[:, : len(degree_tuple)] = degree_tuple\n        components.append(c_mat)\n        d_mat = np.full((len(delay), max_length), -1, dtype=np.int32)\n        d_mat[:, : len(degree_tuple)] = delay\n        delays.append(d_mat)\n        ipc = base[batch] * (\n            base[batch]\n            &gt; self.calc_surr_max(surr=surr[batch], key=degree_tuple, max_scale=max_scale)\n        )\n        ipcs.append(ipc)\n    rank = self.rank\n    if max_length == 0:\n        degrees = np.empty((0,), dtype=np.int32)\n        components = np.empty((0,), dtype=np.int32)\n        delays = np.empty((0,), dtype=np.int32)\n        ipcs = np.empty((0, *rank.shape), dtype=np.float64)\n    else:\n        degrees = np.concatenate(degrees, axis=0)\n        components = np.concatenate(components, axis=0)\n        delays = np.concatenate(delays, axis=0)\n        ipcs = np.concatenate(ipcs, axis=0)\n    if squeeze:\n        batch_shape = [dim for dim in ipcs.shape[1:] if dim != 1]\n        ipcs = ipcs.reshape(-1, *batch_shape)\n        rank = rank.reshape(*batch_shape)\n    df = pl.DataFrame(\n        {\n            \"degree\": np.array(degrees),\n            **{f\"cmp_{idx}\": components[:, idx] for idx in range(max_length)},\n            **{f\"del_{idx}\": delays[:, idx] for idx in range(max_length)},\n            **{\n                f\"ipc_{'_'.join(map(str, idx))}\": ipcs[(slice(None), *idx)]\n                for idx in np.ndindex(*ipcs.shape[1:])\n            },\n        }\n    )\n    if truncate_by_rank:\n        for idx in np.ndindex(*ipcs.shape[1:]):\n            col = f\"ipc_{'_'.join(map(str, idx))}\"\n            df = df.sort(pl.col(col), descending=True)\n            df = df.with_columns(pl.col(col).cum_sum().name.prefix(\"acc_\"))\n            df = df.with_columns(\n                pl.when(pl.col(f\"acc_{col}\") &gt; rank[(*idx,)])\n                .then(0.0)\n                .otherwise(pl.col(col))\n                .alias(col)\n            )\n        df = df.select(~cs.starts_with(\"acc_\")).sort(pl.col(\"degree\"))\n    return df, rank\n</code></pre>"},{"location":"profiler/#ipc_module.profiler.UnivariateViewer.total","title":"<code>total(method=None, max_scale=1.0)</code>","text":"<p>Calculates the total IPCs over all degree tuples stored in the results, optionally filtered by a specified method.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>int | tuple | None</code> <p>Filter method (see <code>filter_by</code> method).</p> <code>None</code> <code>max_scale</code> <code>float</code> <p>Surrogate max scaling factor for IPC thresholding.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>total_ipc</code> <code>ndarray | None</code> <p>Total IPCs over all degree tuples. The shape is equal to the batch shape.</p> <p>Examples:</p> <pre><code>ipc_all = viewer.total()  # Calculate total IPCs for all degree tuples\nipc_3 = viewer.total(method=3)  # Calculate total IPCs for degree sums of 3\nipc_2_1 = viewer.total(method=(2, 1))  # Calculate total IPCs for degree tuple (2, 1)\n</code></pre> Source code in <code>src/ipc_module/profiler.py</code> <pre><code>def total(self, method: int | tuple | None = None, max_scale: float = 1.0):\n    \"\"\"\n    Calculates the total IPCs over all degree tuples stored in the results,\n    optionally filtered by a specified method.\n\n    Parameters:\n        method (int | tuple | None, optional): Filter method (see `filter_by` method).\n        max_scale (float, optional): Surrogate max scaling factor for IPC thresholding.\n\n    Returns:\n        total_ipc (np.ndarray | None): Total IPCs over all degree tuples. The shape is equal to the batch shape.\n\n    Examples:\n        ```python\n        ipc_all = viewer.total()  # Calculate total IPCs for all degree tuples\n        ipc_3 = viewer.total(method=3)  # Calculate total IPCs for degree sums of 3\n        ipc_2_1 = viewer.total(method=(2, 1))  # Calculate total IPCs for degree tuple (2, 1)\n        ```\n    \"\"\"\n    ipcs = []\n    for degree_tuple in self.keys(method):\n        res = self[degree_tuple]\n        if res is None:\n            continue\n        _delay, base, surr = res\n        ipcs.append(\n            base * (base &gt; self.calc_surr_max(surr=surr, key=degree_tuple, max_scale=max_scale))\n        )\n    if len(ipcs) &gt; 0:\n        return np.concatenate(ipcs, axis=0).sum(axis=0)[..., 0]\n</code></pre>"},{"location":"regressor/","title":"regressor","text":""},{"location":"regressor/#ipc_module.regressor.BatchRegressor","title":"<code>BatchRegressor</code>","text":"<p>               Bases: <code>object</code></p> <p>BatchRegressor performs singular value decomposition (SVD) on the provided time series data <code>xs</code> and uses the resulting left singular vectors to compute the regression of polynomial features.</p> Source code in <code>src/ipc_module/regressor.py</code> <pre><code>class BatchRegressor(object):\n    \"\"\"\n    BatchRegressor performs singular value decomposition (SVD) on the provided time series data `xs`\n    and uses the resulting left singular vectors to compute the regression of polynomial features.\n    \"\"\"\n\n    def __init__(self, xs, offset=1000, debias=True, normalize=False, threshold_mode=\"linear\"):\n        \"\"\"\n        Parameters:\n            xs (ndarray): Time series data with shape [*batch_shape, T, D].\n            offset (int, optional): Offset applied to the time series data.\n            debias (bool, optional): Removes the mean from the data if set to True.\n            normalize (bool, optional): Scales the data to unit variance if set to True.\n            threshold_mode (str, optional): Determines the singular value thresholding mode, either 'linear' or 'sqrt'.\n\n        Notes:\n            `BatchRegressor` is used inside `UnivariateProfiler` and is not intended to be used directly by users.\n            The behavior of the regressor can be specified when you create an instance of `UnivariateProfiler`.\n\n        \"\"\"\n        self.backend = import_backend(xs)\n        self.offset, self.length = offset, xs.shape[-2]\n        if debias:\n            xs = xs - xs[..., self.offset :, :].mean(axis=-2, keepdims=True)\n        if normalize:\n            std = backend_std(xs[..., self.offset :, :], axis=-2, keepdims=True)\n            std[std &lt; self.backend.finfo(std.dtype).eps] = 1.0\n            xs = xs / std\n        self.left, self.sigma, _right = self.backend.linalg.svd(\n            xs[..., self.offset :, :], full_matrices=False\n        )\n        # -&gt; [*bs, T, D], [*bs, D], [*bs, D, D]\n        # See https://numpy.org/devdocs/reference/generated/numpy.linalg.matrix_rank.html\n        self.eps = self.backend.finfo(self.sigma.dtype).eps\n        m, n = xs.shape[-2:]\n        sigma_sq_max = backend_max(self.sigma * self.sigma, axis=-1, keepdims=True)  # [*bs, D]\n        if threshold_mode == \"linear\":\n            self.eps = sigma_sq_max * (self.eps * max(m, n))\n        elif threshold_mode == \"sqrt\":\n            self.eps = sigma_sq_max * (self.eps * 0.5 * math.sqrt(m + n + 1))\n        self.mask = (self.sigma &gt; self.eps)[..., None]  # [*bs, D, 1]\n\n    @property\n    def rank(self):\n        return self.mask.sum(axis=(-2, -1))\n\n    def calc(self, y):\n        dot = self.backend.matmul(\n            self.left.swapaxes(-2, -1), y\n        )  # [*bs, T, D], [*bs, T, Y] -&gt; [*bs, D, Y]\n        dot = ((dot * dot) * self.mask).sum(axis=-2)  # [*bs, D, Y], [*bs, D, 1] -&gt; [*bs, Y]\n        var = (y * y).sum(axis=-2)  # [*bs, T, Y] -&gt; [*bs, Y]\n        return dot / var\n\n    def __call__(self, poly: BasePolynomial, degrees: tuple[int], delays: tuple[int], formatter=()):\n        target = functools.reduce(\n            operator.mul,\n            [\n                poly[(deg, *formatter)][..., self.offset - gap : self.length - gap, :]\n                for deg, gap in zip(degrees, delays, strict=False)\n            ],\n            1,\n        )\n        return self.calc(target)\n</code></pre>"},{"location":"regressor/#ipc_module.regressor.BatchRegressor.__init__","title":"<code>__init__(xs, offset=1000, debias=True, normalize=False, threshold_mode='linear')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xs</code> <code>ndarray</code> <p>Time series data with shape [*batch_shape, T, D].</p> required <code>offset</code> <code>int</code> <p>Offset applied to the time series data.</p> <code>1000</code> <code>debias</code> <code>bool</code> <p>Removes the mean from the data if set to True.</p> <code>True</code> <code>normalize</code> <code>bool</code> <p>Scales the data to unit variance if set to True.</p> <code>False</code> <code>threshold_mode</code> <code>str</code> <p>Determines the singular value thresholding mode, either 'linear' or 'sqrt'.</p> <code>'linear'</code> Notes <p><code>BatchRegressor</code> is used inside <code>UnivariateProfiler</code> and is not intended to be used directly by users. The behavior of the regressor can be specified when you create an instance of <code>UnivariateProfiler</code>.</p> Source code in <code>src/ipc_module/regressor.py</code> <pre><code>def __init__(self, xs, offset=1000, debias=True, normalize=False, threshold_mode=\"linear\"):\n    \"\"\"\n    Parameters:\n        xs (ndarray): Time series data with shape [*batch_shape, T, D].\n        offset (int, optional): Offset applied to the time series data.\n        debias (bool, optional): Removes the mean from the data if set to True.\n        normalize (bool, optional): Scales the data to unit variance if set to True.\n        threshold_mode (str, optional): Determines the singular value thresholding mode, either 'linear' or 'sqrt'.\n\n    Notes:\n        `BatchRegressor` is used inside `UnivariateProfiler` and is not intended to be used directly by users.\n        The behavior of the regressor can be specified when you create an instance of `UnivariateProfiler`.\n\n    \"\"\"\n    self.backend = import_backend(xs)\n    self.offset, self.length = offset, xs.shape[-2]\n    if debias:\n        xs = xs - xs[..., self.offset :, :].mean(axis=-2, keepdims=True)\n    if normalize:\n        std = backend_std(xs[..., self.offset :, :], axis=-2, keepdims=True)\n        std[std &lt; self.backend.finfo(std.dtype).eps] = 1.0\n        xs = xs / std\n    self.left, self.sigma, _right = self.backend.linalg.svd(\n        xs[..., self.offset :, :], full_matrices=False\n    )\n    # -&gt; [*bs, T, D], [*bs, D], [*bs, D, D]\n    # See https://numpy.org/devdocs/reference/generated/numpy.linalg.matrix_rank.html\n    self.eps = self.backend.finfo(self.sigma.dtype).eps\n    m, n = xs.shape[-2:]\n    sigma_sq_max = backend_max(self.sigma * self.sigma, axis=-1, keepdims=True)  # [*bs, D]\n    if threshold_mode == \"linear\":\n        self.eps = sigma_sq_max * (self.eps * max(m, n))\n    elif threshold_mode == \"sqrt\":\n        self.eps = sigma_sq_max * (self.eps * 0.5 * math.sqrt(m + n + 1))\n    self.mask = (self.sigma &gt; self.eps)[..., None]  # [*bs, D, 1]\n</code></pre>"}]}